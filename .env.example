# Local LLM SDK Configuration
# Copy this file to .env and update with your settings

# LM Studio/Ollama/LocalAI server URL
LLM_BASE_URL=http://localhost:1234/v1

# Model to use (or "auto" for auto-detection)
LLM_MODEL=your-model-name

# Request timeout in seconds (default: 300)
LLM_TIMEOUT=300

# Enable debug logging (default: false)
LLM_DEBUG=false

# Example configurations for different setups:
#
# LM Studio (local):
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_MODEL=mistralai/magistral-small-2509
#
# LM Studio (network):
# LLM_BASE_URL=http://192.168.1.100:1234/v1
# LLM_MODEL=qwen/qwen3-coder-30b
#
# Ollama (local):
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=mistral:7b
#
# LocalAI:
# LLM_BASE_URL=http://localhost:8080/v1
# LLM_MODEL=your-model
