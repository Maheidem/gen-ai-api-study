{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34363cc4",
   "metadata": {},
   "source": [
    "# LM Studio API Testing Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to interact with LM Studio's OpenAI-compatible API using Python and Pydantic models for type safety.\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio running locally with API server enabled\n",
    "- Python with `requests` and `pydantic` installed\n",
    "- The `api_models.py` file in the same directory\n",
    "\n",
    "## Configuration\n",
    "- **LM Studio URL**: `http://169.254.83.107:1234/v1`\n",
    "- **Test Model**: `mistralai/magistral-small-2509`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35e1ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import requests\n",
    "\n",
    "# Import Pydantic models from our custom module\n",
    "from api_models import (\n",
    "    ModelList,           # Response model for /v1/models endpoint\n",
    "    ModelInfo,           # Individual model information\n",
    "    ChatCompletion,      # Response model for chat completions\n",
    "    ChatCompletionRequest,  # Request model for chat completions\n",
    "    ChatMessage,         # Individual message in a conversation\n",
    "    create_chat_message,  # Helper to create messages\n",
    "    create_chat_completion_request  # Helper to create requests\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z3urlllrtok",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the necessary libraries and Pydantic models from our custom `api_models.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "18bb306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for LM Studio API (OpenAI-compatible format)\n",
    "# Change this to match your LM Studio server address\n",
    "base_url = \"http://169.254.83.107:1234/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjwyuwch46",
   "metadata": {},
   "source": [
    "## 2. Configure API Connection\n",
    "\n",
    "Set up the base URL for your LM Studio instance. This should match the URL shown in LM Studio's server settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f5fc4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OpenAI-compatible endpoints\n",
    "endpoint_models = f\"{base_url}/models\"              # List available models\n",
    "endpoint_chat = f\"{base_url}/chat/completions\"      # Chat completions (main endpoint)\n",
    "endpoint_completions = f\"{base_url}/completions\"    # Legacy completions\n",
    "endpoint_embeddings = f\"{base_url}/embeddings\"      # Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eeg2pown",
   "metadata": {},
   "source": [
    "## 3. Define API Endpoints\n",
    "\n",
    "LM Studio implements OpenAI-compatible endpoints. Here we define the main endpoints we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b740b",
   "metadata": {},
   "source": [
    "## 4. List Available Models\n",
    "\n",
    "Query the API to see which models are currently loaded in LM Studio. This is useful to verify connectivity and see available model IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e8a4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models in LM Studio:\n",
      "----------------------------------------\n",
      "‚Ä¢ mistralai/magistral-small-2509\n",
      "‚Ä¢ qwen/qwen3-coder-30b\n",
      "‚Ä¢ text-embedding-nomic-embed-text-v1.5\n",
      "‚Ä¢ smolvlm2-2.2b-instruct\n",
      "‚Ä¢ google/gemma-3-27b\n",
      "‚Ä¢ text-embedding-mxbai-embed-large-v1\n",
      "\n",
      "Total models loaded: 6\n"
     ]
    }
   ],
   "source": [
    "# Make GET request to list models endpoint\n",
    "resp = requests.get(endpoint_models, timeout=5)\n",
    "\n",
    "# Raise exception if request failed\n",
    "resp.raise_for_status()\n",
    "\n",
    "# Parse response using Pydantic model for type safety\n",
    "models = ModelList.model_validate(resp.json())\n",
    "\n",
    "# Display available models\n",
    "print(\"Available models in LM Studio:\")\n",
    "print(\"-\" * 40)\n",
    "for model in models.data:\n",
    "    print(f\"‚Ä¢ {model.id}\")\n",
    "    \n",
    "print(f\"\\nTotal models loaded: {len(models.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61aa2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat completion request successful!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the conversation messages\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a helpful assistant.\"),\n",
    "    create_chat_message(\"user\", \"Reply to my hello world in a funny way.\")\n",
    "]\n",
    "\n",
    "# Step 2: Create a properly formatted request using Pydantic model\n",
    "chat_request = create_chat_completion_request(\n",
    "    model=\"mistralai/magistral-small-2509\",  # Use model ID from the list above\n",
    "    messages=messages,\n",
    "    temperature=0.7,  # Controls randomness (0=deterministic, 2=very random)\n",
    "    stream=False      # Set True for streaming responses\n",
    ")\n",
    "\n",
    "# Step 3: Send POST request to chat completions endpoint\n",
    "resp = requests.post(\n",
    "    endpoint_chat, \n",
    "    json=chat_request.model_dump(exclude_none=True),  # Convert Pydantic model to JSON\n",
    "    timeout=15  # Allow 15 seconds for response (local models can be slower)\n",
    ")\n",
    "\n",
    "# Step 4: Check if request was successful\n",
    "resp.raise_for_status()\n",
    "\n",
    "# Step 5: Parse and validate response using Pydantic model\n",
    "chat_completion = ChatCompletion.model_validate(resp.json())\n",
    "\n",
    "print(\"‚úÖ Chat completion request successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zvj0d1xc8h",
   "metadata": {},
   "source": [
    "## 5. Send a Chat Completion Request\n",
    "\n",
    "Now let's send a chat completion request to the model. We'll use helper functions to create properly formatted messages and requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "809dc170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model Response:\n",
      "----------------------------------------\n",
      "Hello, world! It's me, your friendly neighborhood chatbot. Ready to assist you like a super-caffeinated squirrel ready to bury an acorn the size of a football. What can I do for you today? üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Extract the assistant's response from the completion object\n",
    "# The response structure: completion -> choices[0] -> message -> content\n",
    "response_text = chat_completion.choices[0].message.content\n",
    "\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(\"-\" * 40)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taslcm7ql3",
   "metadata": {},
   "source": [
    "## 7. Response Metadata\n",
    "\n",
    "Let's examine the complete response structure to understand what information is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "g8nfelh1iq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Token Usage:\n",
      "  ‚Ä¢ Prompt tokens: 21\n",
      "  ‚Ä¢ Completion tokens: 49\n",
      "  ‚Ä¢ Total tokens: 70\n",
      "\n",
      "üìù Metadata:\n",
      "  ‚Ä¢ Model used: mistralai/magistral-small-2509\n",
      "  ‚Ä¢ Request ID: chatcmpl-4ohvj1v13jkc18y0o025m\n",
      "  ‚Ä¢ Finish reason: stop\n"
     ]
    }
   ],
   "source": [
    "# Display token usage information\n",
    "print(\"üìä Token Usage:\")\n",
    "print(f\"  ‚Ä¢ Prompt tokens: {chat_completion.usage.prompt_tokens}\")\n",
    "print(f\"  ‚Ä¢ Completion tokens: {chat_completion.usage.completion_tokens}\")\n",
    "print(f\"  ‚Ä¢ Total tokens: {chat_completion.usage.total_tokens}\")\n",
    "\n",
    "# Display other metadata\n",
    "print(f\"\\nüìù Metadata:\")\n",
    "print(f\"  ‚Ä¢ Model used: {chat_completion.model}\")\n",
    "print(f\"  ‚Ä¢ Request ID: {chat_completion.id}\")\n",
    "print(f\"  ‚Ä¢ Finish reason: {chat_completion.choices[0].finish_reason}\")\n",
    "\n",
    "# LM Studio specific stats (if available)\n",
    "if chat_completion.stats:\n",
    "    print(f\"\\n‚ö° Performance Stats (LM Studio):\")\n",
    "    for key, value in chat_completion.stats.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4tquqspi43u",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the basic workflow for interacting with LM Studio's API:\n",
    "\n",
    "1. **Import models** - Use Pydantic for type-safe API interactions\n",
    "2. **Configure connection** - Set the base URL for your LM Studio instance\n",
    "3. **List models** - Query available models to find the correct ID\n",
    "4. **Create messages** - Build a conversation with system and user messages\n",
    "5. **Send request** - POST to the chat completions endpoint\n",
    "6. **Parse response** - Extract the model's reply and metadata\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- LM Studio implements OpenAI-compatible endpoints\n",
    "- Use Pydantic models for request/response validation\n",
    "- Helper functions simplify message and request creation\n",
    "- Always handle errors with try/except in production code\n",
    "- Local models may be slower than cloud APIs - adjust timeouts accordingly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different models and compare responses\n",
    "- Experiment with temperature and other parameters\n",
    "- Implement streaming for real-time responses\n",
    "- Add error handling and retry logic\n",
    "- Test embeddings generation for semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leb4nsp4txi",
   "metadata": {},
   "source": [
    "## 6. Display the Response\n",
    "\n",
    "Extract and display the model's response from the structured completion object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
