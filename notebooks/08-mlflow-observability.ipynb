{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä 08: MLflow Observability\n",
    "\n",
    "Learn how to use MLflow tracing to observe, debug, and optimize your LLM applications with hierarchical execution traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Install and configure MLflow for tracing\n",
    "- [ ] Start and access the MLflow UI\n",
    "- [ ] Enable automatic tracing in the Local LLM SDK\n",
    "- [ ] Understand hierarchical trace structure (CHAIN ‚Üí LLM ‚Üí AGENT ‚Üí TOOL)\n",
    "- [ ] View and analyze traces in the MLflow UI\n",
    "- [ ] Use traces for debugging complex agent workflows\n",
    "- [ ] Identify performance bottlenecks with trace timing data\n",
    "- [ ] Export and share traces for collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prerequisites\n",
    "\n",
    "- Completed notebook 07 (ReACT Agents)\n",
    "- Understanding of agent execution flow\n",
    "- Familiarity with tools and multi-step tasks\n",
    "- LM Studio running with a model that supports function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Estimated Time: 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What is MLflow Tracing?\n",
    "\n",
    "**MLflow** is an open-source platform for managing ML workflows. **Tracing** is a feature that records the execution of your LLM applications.\n",
    "\n",
    "### Why Tracing?\n",
    "\n",
    "Without tracing:\n",
    "- ‚ùå Can't see what the agent did internally\n",
    "- ‚ùå Hard to debug when things go wrong\n",
    "- ‚ùå No visibility into performance bottlenecks\n",
    "- ‚ùå Difficult to optimize complex workflows\n",
    "\n",
    "With tracing:\n",
    "- ‚úÖ See every LLM call, tool execution, and decision\n",
    "- ‚úÖ Visualize hierarchical execution flow\n",
    "- ‚úÖ Measure timing for each component\n",
    "- ‚úÖ Debug issues by inspecting inputs/outputs at each step\n",
    "- ‚úÖ Compare different runs side-by-side\n",
    "\n",
    "### Hierarchical Trace Structure\n",
    "\n",
    "```\n",
    "CHAIN (top-level task)\n",
    "‚îú‚îÄ‚îÄ LLM (reasoning step 1)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ TOOL (calculator)\n",
    "‚îú‚îÄ‚îÄ LLM (reasoning step 2)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ TOOL (file_write)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ TOOL (file_read)\n",
    "‚îî‚îÄ‚îÄ LLM (reasoning step 3 - conclusion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Installing MLflow\n",
    "\n",
    "First, let's install MLflow if it's not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow (uncomment if needed)\n",
    "# !pip install mlflow>=2.13.0\n",
    "\n",
    "import mlflow\n",
    "\n",
    "print(f\"‚úÖ MLflow version: {mlflow.__version__}\")\n",
    "print(\"\\nüí° MLflow 2.13+ is recommended for best tracing support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Starting the MLflow UI\n",
    "\n",
    "The MLflow UI provides a visual interface for viewing traces.\n",
    "\n",
    "**Start MLflow UI in a terminal:**\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Then open your browser to: **http://localhost:5000**\n",
    "\n",
    "üí° **Tip**: Keep the MLflow UI open in a browser tab while running this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, start it from Python (in background):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check if MLflow UI is already running\n",
    "def is_mlflow_running():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\"http://localhost:5000\", timeout=1)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if is_mlflow_running():\n",
    "    print(\"‚úÖ MLflow UI is already running at http://localhost:5000\")\n",
    "else:\n",
    "    print(\"üöÄ Starting MLflow UI...\")\n",
    "    # Start in background (note: this won't work in all environments)\n",
    "    # It's better to start MLflow UI in a separate terminal\n",
    "    print(\"\\n‚ö†Ô∏è Please start MLflow UI manually in a terminal:\")\n",
    "    print(\"   mlflow ui --port 5000\")\n",
    "    print(\"\\n   Then open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Enabling Automatic Tracing\n",
    "\n",
    "The Local LLM SDK has built-in MLflow tracing support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "from local_llm_sdk.tools import get_builtin_tools\n",
    "\n",
    "# Create client with tracing enabled\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    enable_tracing=True  # Enable MLflow tracing\n",
    ")\n",
    "\n",
    "# Register tools\n",
    "tools = get_builtin_tools()\n",
    "client.register_tools(tools)\n",
    "\n",
    "print(\"‚úÖ Client created with MLflow tracing enabled!\")\n",
    "print(\"\\nüí° All operations will now be traced automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Basic Trace Example\n",
    "\n",
    "Let's make a simple traced call and view it in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat with tracing\n",
    "response = client.chat(\"What is 12 multiplied by 15?\")\n",
    "\n",
    "print(\"üí¨ Response:\")\n",
    "print(response)\n",
    "print(\"\\n‚úÖ This call was traced!\")\n",
    "print(\"\\nüîç View the trace:\")\n",
    "print(\"   1. Go to http://localhost:5000\")\n",
    "print(\"   2. Click on 'Traces' in the sidebar\")\n",
    "print(\"   3. Find the most recent trace\")\n",
    "print(\"   4. Click to see the execution tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ What you'll see in MLflow UI:**\n",
    "\n",
    "- A `CHAIN` span for the overall chat call\n",
    "- An `LLM` span for the model inference\n",
    "- A `TOOL` span for the calculator tool\n",
    "- Input/output for each span\n",
    "- Timing information (duration in ms)\n",
    "- Token counts and other metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Agent Trace - The Full Picture\n",
    "\n",
    "Now let's trace a ReACT agent execution to see the complete hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an agent task with tracing\n",
    "result = client.react(\n",
    "    \"Calculate the factorial of 6, then convert the result to text, \"\n",
    "    \"uppercase it, and count how many characters it has.\",\n",
    "    max_steps=10\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Agent Result:\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Steps: {result.steps_taken}\")\n",
    "print(f\"\\nFinal answer:\\n{result.final_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüîç View the hierarchical trace in MLflow UI:\")\n",
    "print(\"   - CHAIN (agent.react)\")\n",
    "print(\"     ‚îú‚îÄ‚îÄ LLM (reasoning step 1)\")\n",
    "print(\"     ‚îÇ   ‚îî‚îÄ‚îÄ TOOL (execute_python for factorial)\")\n",
    "print(\"     ‚îú‚îÄ‚îÄ LLM (reasoning step 2)\")\n",
    "print(\"     ‚îÇ   ‚îî‚îÄ‚îÄ TOOL (text_transformer)\")\n",
    "print(\"     ‚îú‚îÄ‚îÄ LLM (reasoning step 3)\")\n",
    "print(\"     ‚îÇ   ‚îî‚îÄ‚îÄ TOOL (char_counter)\")\n",
    "print(\"     ‚îî‚îÄ‚îÄ LLM (final answer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Understanding Span Types\n",
    "\n",
    "MLflow uses different span types to categorize operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Type Reference\n",
    "\n",
    "| Span Type | Purpose | Examples |\n",
    "|-----------|---------|----------|\n",
    "| `CHAIN` | High-level workflow | `client.react()`, `client.chat()` |\n",
    "| `LLM` | Model inference | OpenAI/LM Studio API calls |\n",
    "| `AGENT` | Agent reasoning | ReACT loop iterations |\n",
    "| `TOOL` | Tool execution | `math_calculator`, `execute_python` |\n",
    "| `RETRIEVER` | Data retrieval | Database queries, API calls |\n",
    "\n",
    "### Trace Hierarchy\n",
    "\n",
    "```\n",
    "CHAIN (root span - entire workflow)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ AGENT (if using agents)\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ LLM (model call for reasoning)\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attributes: model, temperature, tokens\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ input: messages sent to model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ output: model response\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ TOOL (tool execution)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ attributes: tool name, parameters\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ input: tool arguments\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ output: tool result\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Timing data for each span\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Complex Multi-Tool Trace\n",
    "\n",
    "Let's create a complex workflow to see a rich trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Complex multi-tool task\n",
    "result = client.react(\n",
    "    f\"Generate a Python list of the squares of numbers 1 through 10. \"\n",
    "    f\"Calculate the sum of all those squares. \"\n",
    "    f\"Save the list and sum to {temp_dir}/squares.txt. \"\n",
    "    f\"Then read the file back and count the total characters in it.\",\n",
    "    max_steps=15\n",
    ")\n",
    "\n",
    "print(\"üéØ Complex Workflow Complete!\")\n",
    "print(f\"\\nResult: {result.final_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä Trace Analysis in MLflow UI:\")\n",
    "print(\"\\n1. Navigate to the Traces tab\")\n",
    "print(\"2. Find this trace (most recent)\")\n",
    "print(\"3. Expand the tree to see:\")\n",
    "print(\"   - Multiple LLM calls (one per reasoning step)\")\n",
    "print(\"   - execute_python span (generate squares & sum)\")\n",
    "print(\"   - filesystem_operation spans (write & read)\")\n",
    "print(\"   - char_counter span\")\n",
    "print(\"\\n4. Click each span to inspect:\")\n",
    "print(\"   - Input parameters\")\n",
    "print(\"   - Output values\")\n",
    "print(\"   - Duration (ms)\")\n",
    "print(\"   - Timestamps\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Using Traces for Debugging\n",
    "\n",
    "Traces are invaluable when something goes wrong. Let's see how to debug with traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A task that might have issues\n",
    "result = client.react(\n",
    "    \"Calculate the square root of -1 using Python, \"\n",
    "    \"then tell me what type of number it is.\",\n",
    "    max_steps=8\n",
    ")\n",
    "\n",
    "print(\"üîç Debugging Example:\")\n",
    "print(f\"\\nStatus: {result.status}\")\n",
    "print(f\"\\nResult:\\n{result.final_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüõ†Ô∏è How to Debug with Traces:\")\n",
    "print(\"\\n1. Open the trace in MLflow UI\")\n",
    "print(\"2. Look for spans with errors (red indicators)\")\n",
    "print(\"3. Check the execute_python span:\")\n",
    "print(\"   - Input: What code was executed?\")\n",
    "print(\"   - Output: What was the result/error?\")\n",
    "print(\"4. Trace the reasoning:\")\n",
    "print(\"   - Did the LLM handle complex numbers correctly?\")\n",
    "print(\"   - Did it import the 'cmath' module?\")\n",
    "print(\"5. Identify improvements:\")\n",
    "print(\"   - Better prompting?\")\n",
    "print(\"   - More specific tool instructions?\")\n",
    "print(\"   - Additional tools needed?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Finding Performance Bottlenecks\n",
    "\n",
    "Use timing data in traces to optimize your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task with varying execution times\n",
    "result = client.react(\n",
    "    \"Generate the first 20 prime numbers, \"\n",
    "    \"then calculate statistical properties (mean, median, std dev), \"\n",
    "    \"and finally sort them in reverse order.\",\n",
    "    max_steps=12\n",
    ")\n",
    "\n",
    "print(\"‚ö° Performance Analysis:\")\n",
    "print(f\"\\nTotal steps: {result.steps_taken}\")\n",
    "print(f\"Result: {result.final_response[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìà Performance Optimization with Traces:\")\n",
    "print(\"\\n1. Open trace in MLflow UI\")\n",
    "print(\"2. Sort spans by duration\")\n",
    "print(\"3. Identify slow operations:\")\n",
    "print(\"   - Which LLM calls took longest?\")\n",
    "print(\"   - Which tools were slowest?\")\n",
    "print(\"   - Are there redundant calls?\")\n",
    "print(\"\\n4. Optimization strategies:\")\n",
    "print(\"   - Combine multiple tool calls into one\")\n",
    "print(\"   - Cache results of expensive operations\")\n",
    "print(\"   - Use faster models for simple reasoning\")\n",
    "print(\"   - Reduce max_steps if agent is over-thinking\")\n",
    "print(\"\\n5. Compare before/after:\")\n",
    "print(\"   - Run optimized version\")\n",
    "print(\"   - Compare traces side-by-side\")\n",
    "print(\"   - Measure improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Trace Metadata and Custom Attributes\n",
    "\n",
    "You can add custom metadata to traces for better organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set run name and tags for better organization\n",
    "mlflow.set_experiment(\"Local LLM SDK Tutorial\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"fibonacci-research\") as run:\n",
    "    # Tag the run\n",
    "    mlflow.set_tag(\"task_type\", \"mathematical_research\")\n",
    "    mlflow.set_tag(\"complexity\", \"medium\")\n",
    "    \n",
    "    # Execute agent\n",
    "    result = client.react(\n",
    "        \"Calculate the first 10 Fibonacci numbers, \"\n",
    "        \"sum them, and determine if the sum is prime.\",\n",
    "        max_steps=10\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"steps_taken\", result.steps_taken)\n",
    "    mlflow.log_metric(\"success\", 1 if result.status == \"success\" else 0)\n",
    "    \n",
    "    print(\"‚úÖ Trace logged with metadata!\")\n",
    "    print(f\"\\nRun ID: {run.info.run_id}\")\n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"\\nResult: {result.final_response}\")\n",
    "\n",
    "print(\"\\nüí° View in MLflow UI:\")\n",
    "print(\"   - Experiments tab: See all runs organized by experiment\")\n",
    "print(\"   - Filter by tags: complexity=medium\")\n",
    "print(\"   - Compare metrics across runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise: Trace Analysis Challenge\n",
    "\n",
    "**Challenge:** Create and analyze traces for a data processing pipeline.\n",
    "\n",
    "**Task:**\n",
    "1. Create an agent that processes a list of numbers\n",
    "2. The agent should:\n",
    "   - Generate 25 random numbers (1-100)\n",
    "   - Filter for even numbers only\n",
    "   - Calculate mean and median of even numbers\n",
    "   - Save results to a file\n",
    "   - Count characters in the saved file\n",
    "\n",
    "**Analysis Requirements:**\n",
    "1. Run the task and examine the trace in MLflow UI\n",
    "2. Count total spans in the trace\n",
    "3. Identify the slowest span\n",
    "4. Find how many tool calls were made\n",
    "5. Calculate total execution time\n",
    "\n",
    "Try it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Trace Analysis Challenge\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "results_file = os.path.join(temp_dir, \"even_numbers_analysis.txt\")\n",
    "\n",
    "print(\"üìä Data Processing Pipeline with Tracing\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set up organized tracing\n",
    "mlflow.set_experiment(\"Data Processing Tutorial\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"even-numbers-pipeline\") as run:\n",
    "    # Tag for organization\n",
    "    mlflow.set_tag(\"pipeline_type\", \"data_processing\")\n",
    "    mlflow.set_tag(\"data_size\", \"25_numbers\")\n",
    "    \n",
    "    # Execute the pipeline\n",
    "    result = client.react(\n",
    "        f\"Generate 25 random integers between 1 and 100. \"\n",
    "        f\"Filter to keep only even numbers. \"\n",
    "        f\"Calculate the mean and median of the even numbers. \"\n",
    "        f\"Save all results (original list, even list, mean, median) to {results_file}. \"\n",
    "        f\"Then count how many characters are in that file.\",\n",
    "        max_steps=15\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"steps_taken\", result.steps_taken)\n",
    "    mlflow.log_metric(\"success\", 1 if result.status == \"success\" else 0)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline Status: {result.status}\")\n",
    "    print(f\"Steps taken: {result.steps_taken}\")\n",
    "    print(f\"\\nFinal Result:\\n{result.final_response}\")\n",
    "    \n",
    "    # Verify output file\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        print(f\"\\nüìÑ Generated File ({len(content)} chars):\\n{content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüîç Trace Analysis Instructions:\\n\")\n",
    "    print(\"1. Open MLflow UI: http://localhost:5000\")\n",
    "    print(\"2. Navigate to 'Experiments' ‚Üí 'Data Processing Tutorial'\")\n",
    "    print(f\"3. Find run: 'even-numbers-pipeline' (ID: {run.info.run_id[:8]}...)\")\n",
    "    print(\"4. Click 'Traces' to view the execution tree\")\n",
    "    print(\"\\nüìä Analysis Tasks:\")\n",
    "    print(\"   ‚ñ° Count total spans (expand all nodes)\")\n",
    "    print(\"   ‚ñ° Identify slowest span (check duration column)\")\n",
    "    print(\"   ‚ñ° Count TOOL spans (how many tool calls?)\")\n",
    "    print(\"   ‚ñ° Note total execution time (root CHAIN span)\")\n",
    "    print(\"   ‚ñ° Inspect inputs/outputs of each span\")\n",
    "    print(\"\\nüí° Expected Observations:\")\n",
    "    print(\"   - Multiple LLM spans (one per reasoning step)\")\n",
    "    print(\"   - execute_python spans (random numbers, filtering, stats)\")\n",
    "    print(\"   - filesystem_operation spans (write and read)\")\n",
    "    print(\"   - char_counter span (final count)\")\n",
    "    print(\"   - Each span shows exact inputs and outputs\")\n",
    "    print(\"   - Duration shows which operations are slowest\")\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"\\n‚úÖ Analysis complete!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution cell (run to see answer)\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "results_file = os.path.join(temp_dir, \"even_numbers_analysis.txt\")\n",
    "\n",
    "print(\"üìä Data Processing Pipeline with Tracing\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mlflow.set_experiment(\"Data Processing Tutorial\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"even-numbers-pipeline\") as run:\n",
    "    mlflow.set_tag(\"pipeline_type\", \"data_processing\")\n",
    "    mlflow.set_tag(\"data_size\", \"25_numbers\")\n",
    "    \n",
    "    result = client.react(\n",
    "        f\"Generate 25 random integers between 1 and 100. \"\n",
    "        f\"Filter to keep only even numbers. \"\n",
    "        f\"Calculate the mean and median of the even numbers. \"\n",
    "        f\"Save all results (original list, even list, mean, median) to {results_file}. \"\n",
    "        f\"Then count how many characters are in that file.\",\n",
    "        max_steps=15\n",
    "    )\n",
    "    \n",
    "    mlflow.log_metric(\"steps_taken\", result.steps_taken)\n",
    "    mlflow.log_metric(\"success\", 1 if result.status == \"success\" else 0)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline Status: {result.status}\")\n",
    "    print(f\"Steps taken: {result.steps_taken}\")\n",
    "    print(f\"\\nFinal Result:\\n{result.final_response}\")\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        print(f\"\\nüìÑ Generated File ({len(content)} chars):\\n{content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüîç Trace Analysis Instructions:\\n\")\n",
    "    print(\"1. Open MLflow UI: http://localhost:5000\")\n",
    "    print(\"2. Navigate to 'Experiments' ‚Üí 'Data Processing Tutorial'\")\n",
    "    print(f\"3. Find run: 'even-numbers-pipeline' (ID: {run.info.run_id[:8]}...)\")\n",
    "    print(\"4. Click 'Traces' to view the execution tree\")\n",
    "    print(\"\\nüìä Analysis Tasks:\")\n",
    "    print(\"   ‚ñ° Count total spans (expand all nodes)\")\n",
    "    print(\"   ‚ñ° Identify slowest span (check duration column)\")\n",
    "    print(\"   ‚ñ° Count TOOL spans (how many tool calls?)\")\n",
    "    print(\"   ‚ñ° Note total execution time (root CHAIN span)\")\n",
    "    print(\"   ‚ñ° Inspect inputs/outputs of each span\")\n",
    "\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Enable Tracing\n",
    "```python\n",
    "# ‚ùå Bad: Tracing not enabled\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "# No traces will be generated\n",
    "\n",
    "# ‚úÖ Good: Enable tracing\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"...\",\n",
    "    model=\"...\",\n",
    "    enable_tracing=True\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Not Starting MLflow UI\n",
    "```bash\n",
    "# ‚ö†Ô∏è Must start MLflow UI to view traces\n",
    "mlflow ui --port 5000\n",
    "\n",
    "# Then open http://localhost:5000\n",
    "```\n",
    "\n",
    "### 3. Overwhelming Trace Volume\n",
    "```python\n",
    "# ‚ö†Ô∏è Too many traces can clutter the UI\n",
    "for i in range(1000):\n",
    "    client.chat(f\"Task {i}\")  # Creates 1000 traces!\n",
    "\n",
    "# üí° Tip: Use experiments and tags to organize\n",
    "# üí° Tip: Disable tracing for production/bulk operations\n",
    "```\n",
    "\n",
    "### 4. Not Using Experiments\n",
    "```python\n",
    "# ‚ùå Bad: All traces in default experiment\n",
    "client.chat(\"Task 1\")\n",
    "client.chat(\"Task 2\")\n",
    "# Hard to find specific traces later\n",
    "\n",
    "# ‚úÖ Good: Organize with experiments\n",
    "mlflow.set_experiment(\"Tutorial Examples\")\n",
    "with mlflow.start_run(run_name=\"specific-task\"):\n",
    "    client.chat(\"Task\")\n",
    "```\n",
    "\n",
    "### 5. Ignoring Trace Insights\n",
    "```python\n",
    "# ‚ö†Ô∏è Don't just collect traces - analyze them!\n",
    "\n",
    "# Look for:\n",
    "# - Slow operations (optimize or cache)\n",
    "# - Redundant tool calls (combine operations)\n",
    "# - Error patterns (improve error handling)\n",
    "# - Token usage (optimize prompts)\n",
    "# - Reasoning quality (improve instructions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You Learned\n",
    "\n",
    "‚úÖ **MLflow Setup**: Installing and starting the MLflow UI\n",
    "\n",
    "‚úÖ **Automatic Tracing**: Enabling tracing with `enable_tracing=True`\n",
    "\n",
    "‚úÖ **Hierarchical Traces**: Understanding CHAIN ‚Üí LLM ‚Üí AGENT ‚Üí TOOL structure\n",
    "\n",
    "‚úÖ **Trace Inspection**: Viewing inputs, outputs, and timing in MLflow UI\n",
    "\n",
    "‚úÖ **Debugging**: Using traces to understand what went wrong\n",
    "\n",
    "‚úÖ **Performance**: Identifying bottlenecks with timing data\n",
    "\n",
    "‚úÖ **Organization**: Using experiments, runs, and tags\n",
    "\n",
    "‚úÖ **Best Practices**: When and how to use tracing effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "You've mastered observability with MLflow! Now let's learn production-ready patterns.\n",
    "\n",
    "‚û°Ô∏è Continue to [09-production-patterns.ipynb](./09-production-patterns.ipynb) to learn:\n",
    "- Error handling and retry logic\n",
    "- Timeout configuration\n",
    "- Exponential backoff strategies\n",
    "- Environment-specific settings\n",
    "- Logging best practices\n",
    "- Building robust API wrappers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}