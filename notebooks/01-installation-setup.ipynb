{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Installation & Setup\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Install the Local LLM SDK\n",
    "- ‚úÖ Connect to a local LLM server (LM Studio)\n",
    "- ‚úÖ Configure the SDK using environment variables\n",
    "- ‚úÖ Verify your setup is working correctly\n",
    "- ‚úÖ Troubleshoot common connection issues\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Python 3.9+ installed\n",
    "- LM Studio or compatible OpenAI-spec server running locally\n",
    "- Basic Python knowledge\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install the SDK\n",
    "\n",
    "The Local LLM SDK is a Python package that provides a clean, type-safe interface for interacting with local LLM servers.\n",
    "\n",
    "**Install from the repository:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Legacy editable install of local-llm-sdk==0.1.0 from file:///home/maheidem/gen-ai-api-study (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m‚úÖ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install in development mode (run from notebooks directory)\n",
    "!pip install -e .. --quiet\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import and Verify\n",
    "\n",
    "Let's verify the SDK is installed correctly by importing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Warning: Could not auto-detect model (HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d0e182dca40>: Failed to establish a new connection: [Errno 111] Connection refused'))). You'll need to specify model per request.\n",
      "‚úÖ Local LLM SDK v0.1.0\n",
      "üì¶ Package location: /home/maheidem/gen-ai-api-study/local_llm_sdk/__init__.py\n"
     ]
    }
   ],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "import local_llm_sdk\n",
    "\n",
    "print(f\"‚úÖ Local LLM SDK v{local_llm_sdk.__version__}\")\n",
    "print(f\"üì¶ Package location: {local_llm_sdk.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Connection\n",
    "\n",
    "The SDK needs to know where your LLM server is running. You have three options:\n",
    "\n",
    "### Option A: Environment Variables (Recommended for Development)\n",
    "```bash\n",
    "export LLM_BASE_URL=\"http://localhost:1234/v1\"\n",
    "export LLM_MODEL=\"mistralai/magistral-small-2509\"\n",
    "export LLM_TIMEOUT=\"300\"\n",
    "```\n",
    "\n",
    "### Option B: Direct in Code (Quick Testing)\n",
    "```python\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    timeout=300\n",
    ")\n",
    "```\n",
    "\n",
    "### Option C: .env File (Production)\n",
    "Create a `.env` file with your settings (requires python-dotenv)\n",
    "\n",
    "**For this tutorial, we'll use Option B (direct in code):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to: http://169.254.83.107:1234/v1\n",
      "ü§ñ Using model: mistralai/magistral-small-2509\n"
     ]
    }
   ],
   "source": [
    "# Configure your connection here\n",
    "# ‚ö†Ô∏è UPDATE THIS: Change to your actual LM Studio address and model\n",
    "\n",
    "BASE_URL = \"http://169.254.83.107:1234/v1\"  # Default LM Studio address\n",
    "MODEL = \"mistralai/magistral-small-2509\"  # Will auto-detect first available model\n",
    "\n",
    "print(f\"üîå Connecting to: {BASE_URL}\")\n",
    "print(f\"ü§ñ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Client and Test Connection\n",
    "\n",
    "Now let's create a client and test the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created successfully!\n",
      "   Base URL: http://169.254.83.107:1234/v1\n",
      "   Default Model: mistralai/magistral-small-2509\n",
      "   Timeout: 300s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create client\n",
    "    client = LocalLLMClient(\n",
    "        base_url=BASE_URL,\n",
    "        model=MODEL\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Client created successfully!\")\n",
    "    print(f\"   Base URL: {client.base_url}\")\n",
    "    print(f\"   Default Model: {client.default_model}\")\n",
    "    print(f\"   Timeout: {client.timeout}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüîç Troubleshooting tips:\")\n",
    "    print(\"  1. Is LM Studio running?\")\n",
    "    print(\"  2. Is the API server enabled in LM Studio?\")\n",
    "    print(\"  3. Check if the BASE_URL is correct\")\n",
    "    print(\"  4. Is a model loaded in LM Studio?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: List Available Models\n",
    "\n",
    "Let's see what models are available on your server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Available Models:\n",
      "============================================================\n",
      "1. qwen/qwen3-coder-30b\n",
      "   Owner: organization_owner\n",
      "\n",
      "2. mistralai/magistral-small-2509\n",
      "   Owner: organization_owner\n",
      "\n",
      "3. text-embedding-nomic-embed-text-v1.5\n",
      "   Owner: organization_owner\n",
      "\n",
      "4. smolvlm2-2.2b-instruct\n",
      "   Owner: organization_owner\n",
      "\n",
      "5. google/gemma-3-27b\n",
      "   Owner: organization_owner\n",
      "\n",
      "6. text-embedding-mxbai-embed-large-v1\n",
      "   Owner: organization_owner\n",
      "============================================================\n",
      "Total: 6 model(s) available\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    models = client.list_models()\n",
    "    \n",
    "    print(\"üì¶ Available Models:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for idx, model in enumerate(models.data, 1):\n",
    "        print(f\"{idx}. {model.id}\")\n",
    "        print(f\"   Owner: {model.owned_by}\")\n",
    "        if idx < len(models.data):\n",
    "            print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(models.data)} model(s) available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to list models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Send Your First Message\n",
    "\n",
    "Let's verify everything works by sending a simple message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ SUCCESS! Your setup is working correctly.\n",
      "\n",
      "ü§ñ Model Response:\n",
      "============================================================\n",
      "Hello, world!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.chat(\"Hello! Can you respond with just 'Hello, world!'?\")\n",
    "    \n",
    "    print(\"üéâ SUCCESS! Your setup is working correctly.\")\n",
    "    print(\"\\nü§ñ Model Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Chat failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise: Configure Your Environment\n",
    "\n",
    "**Task**: Set up environment variables for your system\n",
    "\n",
    "1. Find your LM Studio server address (usually `http://localhost:1234/v1`)\n",
    "2. Note the model name you want to use by default\n",
    "3. Set these as environment variables\n",
    "\n",
    "**On Linux/Mac:**\n",
    "```bash\n",
    "export LLM_BASE_URL=\"http://localhost:1234/v1\"\n",
    "export LLM_MODEL=\"your-model-name\"\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```cmd\n",
    "set LLM_BASE_URL=http://localhost:1234/v1\n",
    "set LLM_MODEL=your-model-name\n",
    "```\n",
    "\n",
    "Then create a client without arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this after setting environment variables:\n",
    "# client = LocalLLMClient()  # Uses env vars automatically\n",
    "# print(f\"Using: {client.base_url} with {client.default_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Issues and Solutions\n",
    "\n",
    "### Issue 1: \"Connection refused\"\n",
    "**Solution**: Make sure LM Studio is running and the API server is enabled\n",
    "- Open LM Studio\n",
    "- Go to Developer tab\n",
    "- Click \"Start Server\"\n",
    "\n",
    "### Issue 2: \"No models found\"\n",
    "**Solution**: Load a model in LM Studio\n",
    "- Go to Models tab\n",
    "- Download a model (e.g., Mistral, Llama)\n",
    "- Click \"Load\" to activate it\n",
    "\n",
    "### Issue 3: \"Timeout error\"\n",
    "**Solution**: Increase timeout for slower models\n",
    "```python\n",
    "client = LocalLLMClient(timeout=600)  # 10 minutes\n",
    "```\n",
    "\n",
    "### Issue 4: \"Model not responding\"\n",
    "**Solution**: Check model parameters in LM Studio\n",
    "- Ensure context length isn't exceeded\n",
    "- Try a smaller model if RAM is limited\n",
    "- Check LM Studio logs for errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ What You Learned\n",
    "\n",
    "Congratulations! You now know how to:\n",
    "- ‚úÖ Install the Local LLM SDK\n",
    "- ‚úÖ Configure connection to a local LLM server\n",
    "- ‚úÖ Create a client and verify connectivity\n",
    "- ‚úÖ List available models\n",
    "- ‚úÖ Send your first chat message\n",
    "- ‚úÖ Troubleshoot common connection issues\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "Now that your setup is working, you're ready to learn about basic chat interactions:\n",
    "\n",
    "**Next notebook:** [02-basic-chat.ipynb](02-basic-chat.ipynb)\n",
    "\n",
    "You'll learn:\n",
    "- Simple chat patterns\n",
    "- System prompts\n",
    "- Temperature control\n",
    "- Response formats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
