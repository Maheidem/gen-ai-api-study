{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí≠ 03: Conversation History\n",
    "\n",
    "Learn how to maintain context across multiple messages to build natural, coherent conversations with your local LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Build multi-turn conversations with context\n",
    "- [ ] Use `chat_with_history()` to maintain conversation state\n",
    "- [ ] Understand the role of message history in LLM responses\n",
    "- [ ] Recognize when context matters (and when it doesn't)\n",
    "- [ ] Manage conversation history efficiently\n",
    "- [ ] Build a complete Q&A session with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prerequisites\n",
    "\n",
    "- Completed notebook 02 (Basic Chat)\n",
    "- Understanding of system prompts and temperature\n",
    "- LM Studio running with a loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Estimated Time: 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ The Problem: No Memory in Simple Chat\n",
    "\n",
    "Each `chat()` call is independent - the LLM doesn't remember previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\"\n",
    ")\n",
    "\n",
    "# First message\n",
    "response1 = client.chat(\"My name is Alice.\")\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "# Second message - but the LLM won't remember!\n",
    "response2 = client.chat(\"What's my name?\")\n",
    "print(\"\\nResponse 2:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ùå The LLM can't remember your name because each call is independent!**\n",
    "\n",
    "Each `chat()` call creates a new conversation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ The Solution: Conversation History\n",
    "\n",
    "Use `chat_with_history()` to maintain context across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation with history\n",
    "history = []\n",
    "\n",
    "# Turn 1: Introduce yourself\n",
    "response1, history = client.chat_with_history(\n",
    "    \"My name is Alice.\",\n",
    "    history\n",
    ")\n",
    "print(\"Turn 1:\")\n",
    "print(f\"You: My name is Alice.\")\n",
    "print(f\"LLM: {response1}\")\n",
    "\n",
    "# Turn 2: Ask about your name\n",
    "response2, history = client.chat_with_history(\n",
    "    \"What's my name?\",\n",
    "    history\n",
    ")\n",
    "print(\"\\nTurn 2:\")\n",
    "print(f\"You: What's my name?\")\n",
    "print(f\"LLM: {response2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\nüìö Conversation history now has {len(history)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚úÖ Now the LLM remembers!**\n",
    "\n",
    "The `chat_with_history()` method:\n",
    "1. Takes your new message\n",
    "2. Adds it to the conversation history\n",
    "3. Sends the full history to the LLM\n",
    "4. Returns both the response AND the updated history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Understanding Message History\n",
    "\n",
    "Let's peek inside the history to see what's actually stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"üìù Current conversation history:\\n\")\n",
    "for i, message in enumerate(history, 1):\n",
    "    print(f\"Message {i}:\")\n",
    "    print(f\"  Role: {message['role']}\")\n",
    "    print(f\"  Content: {message['content']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Message Structure:**\n",
    "\n",
    "Each message has two key fields:\n",
    "- `role`: Who sent the message (\"user\", \"assistant\", or \"system\")\n",
    "- `content`: The actual text of the message\n",
    "\n",
    "The conversation alternates: user ‚Üí assistant ‚Üí user ‚Üí assistant ‚Üí ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Building a Multi-Turn Conversation\n",
    "\n",
    "Let's have a real conversation with context building over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh\n",
    "history = []\n",
    "\n",
    "# Define a conversation\n",
    "conversation = [\n",
    "    \"I'm planning a trip to Japan.\",\n",
    "    \"What's the best time of year to visit?\",\n",
    "    \"I love food. What should I try?\",\n",
    "    \"How much would typical street food cost?\",\n",
    "    \"Can you summarize your travel advice for me?\"\n",
    "]\n",
    "\n",
    "print(\"üóæ Japan Travel Planning Conversation\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for user_message in conversation:\n",
    "    # Send message with history\n",
    "    response, history = client.chat_with_history(\n",
    "        user_message,\n",
    "        history\n",
    "    )\n",
    "    \n",
    "    # Display the exchange\n",
    "    print(f\"üë§ You: {user_message}\")\n",
    "    print(f\"ü§ñ LLM: {response}\")\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"\\nüìö Final conversation: {len(history)} messages total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Notice how the LLM:**\n",
    "- Remembers you're planning a Japan trip\n",
    "- Connects your love of food to restaurant recommendations\n",
    "- Provides cost estimates in context of food discussion\n",
    "- Summarizes the entire conversation at the end\n",
    "\n",
    "This is only possible because we maintained the conversation history!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ When Context Matters (and When It Doesn't)\n",
    "\n",
    "Context is important for some tasks but unnecessary for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ WHEN CONTEXT MATTERS:\\n\")\n",
    "\n",
    "# Example: Refining requirements\n",
    "history = []\n",
    "\n",
    "msg1 = \"I need a function to process data.\"\n",
    "resp1, history = client.chat_with_history(msg1, history)\n",
    "print(f\"You: {msg1}\")\n",
    "print(f\"LLM: {resp1[:100]}...\\n\")\n",
    "\n",
    "msg2 = \"It should handle CSV files.\"\n",
    "resp2, history = client.chat_with_history(msg2, history)\n",
    "print(f\"You: {msg2}\")\n",
    "print(f\"LLM: {resp2[:100]}...\\n\")\n",
    "\n",
    "msg3 = \"And it needs to filter rows where age > 18.\"\n",
    "resp3, history = client.chat_with_history(msg3, history)\n",
    "print(f\"You: {msg3}\")\n",
    "print(f\"LLM: {resp3[:100]}...\\n\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(\"‚ùå WHEN CONTEXT DOESN'T MATTER:\\n\")\n",
    "\n",
    "# Example: Independent factual queries\n",
    "fact1 = client.chat(\"What is the capital of France?\")\n",
    "print(f\"Q: What is the capital of France?\")\n",
    "print(f\"A: {fact1}\\n\")\n",
    "\n",
    "fact2 = client.chat(\"What is 25 * 4?\")\n",
    "print(f\"Q: What is 25 * 4?\")\n",
    "print(f\"A: {fact2}\\n\")\n",
    "\n",
    "fact3 = client.chat(\"Translate 'hello' to Spanish.\")\n",
    "print(f\"Q: Translate 'hello' to Spanish.\")\n",
    "print(f\"A: {fact3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Use context when:**\n",
    "- Building on previous responses\n",
    "- Refining requirements or ideas\n",
    "- Having natural conversations\n",
    "- Working on a single topic over multiple turns\n",
    "- When \"it\" or \"that\" refers to earlier messages\n",
    "\n",
    "**üí° Skip context when:**\n",
    "- Asking independent factual questions\n",
    "- Performing simple calculations\n",
    "- Doing batch operations on unrelated items\n",
    "- Testing different prompts on the same task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Managing History: System Prompts\n",
    "\n",
    "You can include a system prompt when starting a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system prompt\n",
    "history = []\n",
    "system_prompt = \"You are a helpful Python tutor who explains concepts simply with code examples.\"\n",
    "\n",
    "# First message - include system prompt\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What is a list comprehension?\",\n",
    "    history,\n",
    "    system=system_prompt\n",
    ")\n",
    "print(\"Turn 1:\")\n",
    "print(response1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Subsequent messages - no need to repeat system prompt!\n",
    "response2, history = client.chat_with_history(\n",
    "    \"Show me a more complex example.\",\n",
    "    history\n",
    ")\n",
    "print(\"Turn 2:\")\n",
    "print(response2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"üìù History structure:\")\n",
    "for msg in history:\n",
    "    print(f\"  - {msg['role']}: {msg['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Important:** Only include `system` parameter on the FIRST call. The system message becomes part of the history and persists automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Advanced: Inspecting and Modifying History\n",
    "\n",
    "You can manually inspect or modify the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "history = []\n",
    "response, history = client.chat_with_history(\"My favorite color is blue.\", history)\n",
    "response, history = client.chat_with_history(\"My favorite food is pizza.\", history)\n",
    "response, history = client.chat_with_history(\"What are my favorites?\", history)\n",
    "\n",
    "print(\"Original response:\", response)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Manually remove the food preference from history\n",
    "# (Keep only system, first user msg, first assistant msg, and last question)\n",
    "modified_history = [history[0], history[1], history[-1]]\n",
    "\n",
    "# Ask again with modified history\n",
    "response2, _ = client.chat_with_history(\n",
    "    \"What are my favorites?\",\n",
    "    modified_history\n",
    ")\n",
    "\n",
    "print(\"Response with modified history:\", response2)\n",
    "print(\"\\nüí° Now it only remembers your color preference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Advanced technique:** Manually editing history is useful for:\n",
    "- Removing sensitive information\n",
    "- Keeping only relevant context (token optimization)\n",
    "- Correcting mistakes in the conversation\n",
    "- Implementing sliding window context (keep last N messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise: Build a Quiz Game\n",
    "\n",
    "**Challenge:** Create a Q&A quiz session where:\n",
    "1. The LLM asks you 3 trivia questions (one at a time)\n",
    "2. You provide answers\n",
    "3. The LLM evaluates each answer\n",
    "4. At the end, the LLM summarizes your score\n",
    "\n",
    "Requirements:\n",
    "- Use `chat_with_history()` to maintain context\n",
    "- Use a system prompt to make the LLM a quiz host\n",
    "- Must have at least 5+ turns (intro, Q1, A1, Q2, A2, Q3, A3, summary)\n",
    "\n",
    "Try it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Quiz game with conversation history\n",
    "\n",
    "history = []\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly quiz show host. Ask the user 3 trivia questions, one at a time.\n",
    "After each answer, tell them if they're correct and give the right answer if they're wrong.\n",
    "After all 3 questions, summarize their score.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üéÆ TRIVIA QUIZ GAME\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Start the quiz\n",
    "response, history = client.chat_with_history(\n",
    "    \"Let's start the quiz!\",\n",
    "    history,\n",
    "    system=system_prompt\n",
    ")\n",
    "print(f\"üé§ Host: {response}\\n\")\n",
    "\n",
    "# User answers (you can modify these)\n",
    "answers = [\n",
    "    \"Paris\",           # If question is about France's capital\n",
    "    \"I don't know\",    # Skip a question\n",
    "    \"42\"               # Random guess\n",
    "]\n",
    "\n",
    "for i, answer in enumerate(answers, 1):\n",
    "    # Provide answer\n",
    "    print(f\"üë§ You: {answer}\")\n",
    "    response, history = client.chat_with_history(answer, history)\n",
    "    print(f\"üé§ Host: {response}\\n\")\n",
    "    print(\"-\"*70 + \"\\n\")\n",
    "\n",
    "# Get final summary\n",
    "if \"score\" not in response.lower() and \"how did\" not in response.lower():\n",
    "    response, history = client.chat_with_history(\n",
    "        \"Can you tell me my final score?\",\n",
    "        history\n",
    "    )\n",
    "    print(f\"üë§ You: Can you tell me my final score?\")\n",
    "    print(f\"üé§ Host: {response}\")\n",
    "\n",
    "print(f\"\\n\\nüìä Total conversation turns: {len(history)}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution cell (run this to see the answer)\n",
    "history = []\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly quiz show host. Ask the user 3 trivia questions, one at a time.\n",
    "After each answer, tell them if they're correct and give the right answer if they're wrong.\n",
    "After all 3 questions, summarize their score.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üéÆ TRIVIA QUIZ GAME\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Start the quiz\n",
    "response, history = client.chat_with_history(\n",
    "    \"Let's start the quiz!\",\n",
    "    history,\n",
    "    system=system_prompt\n",
    ")\n",
    "print(f\"üé§ Host: {response}\\n\")\n",
    "\n",
    "# User answers (you can modify these)\n",
    "answers = [\n",
    "    \"Paris\",\n",
    "    \"I don't know\",\n",
    "    \"42\"\n",
    "]\n",
    "\n",
    "for i, answer in enumerate(answers, 1):\n",
    "    print(f\"üë§ You: {answer}\")\n",
    "    response, history = client.chat_with_history(answer, history)\n",
    "    print(f\"üé§ Host: {response}\\n\")\n",
    "    print(\"-\"*70 + \"\\n\")\n",
    "\n",
    "# Get final summary\n",
    "if \"score\" not in response.lower() and \"how did\" not in response.lower():\n",
    "    response, history = client.chat_with_history(\n",
    "        \"Can you tell me my final score?\",\n",
    "        history\n",
    "    )\n",
    "    print(f\"üë§ You: Can you tell me my final score?\")\n",
    "    print(f\"üé§ Host: {response}\")\n",
    "\n",
    "print(f\"\\n\\nüìä Total conversation turns: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Update History\n",
    "```python\n",
    "# ‚ùå Bad: Not using the returned history\n",
    "history = []\n",
    "response, history = client.chat_with_history(\"Hello\", history)\n",
    "response, _ = client.chat_with_history(\"What did I just say?\", history)  # Wrong!\n",
    "# The second message won't remember \"Hello\"\n",
    "\n",
    "# ‚úÖ Good: Always update history\n",
    "history = []\n",
    "response, history = client.chat_with_history(\"Hello\", history)\n",
    "response, history = client.chat_with_history(\"What did I just say?\", history)\n",
    "```\n",
    "\n",
    "### 2. Repeating System Prompts\n",
    "```python\n",
    "# ‚ùå Bad: Adding system prompt on every turn\n",
    "history = []\n",
    "system = \"You are a helpful assistant.\"\n",
    "response, history = client.chat_with_history(\"Hi\", history, system=system)\n",
    "response, history = client.chat_with_history(\"How are you?\", history, system=system)\n",
    "# This adds duplicate system messages!\n",
    "\n",
    "# ‚úÖ Good: System prompt only on first turn\n",
    "history = []\n",
    "response, history = client.chat_with_history(\"Hi\", history, system=system)\n",
    "response, history = client.chat_with_history(\"How are you?\", history)\n",
    "```\n",
    "\n",
    "### 3. Using History When Not Needed\n",
    "```python\n",
    "# ‚ùå Bad: Using history for independent tasks\n",
    "history = []\n",
    "response, history = client.chat_with_history(\"What is 2+2?\", history)\n",
    "response, history = client.chat_with_history(\"What is 5*5?\", history)\n",
    "response, history = client.chat_with_history(\"What is 10-3?\", history)\n",
    "# Unnecessary context, wastes tokens\n",
    "\n",
    "# ‚úÖ Good: Use simple chat for independent queries\n",
    "response1 = client.chat(\"What is 2+2?\")\n",
    "response2 = client.chat(\"What is 5*5?\")\n",
    "response3 = client.chat(\"What is 10-3?\")\n",
    "```\n",
    "\n",
    "### 4. Not Managing History Size\n",
    "```python\n",
    "# ‚ö†Ô∏è Warning: Long conversations consume many tokens\n",
    "history = []\n",
    "for i in range(100):  # Very long conversation\n",
    "    response, history = client.chat_with_history(f\"Message {i}\", history)\n",
    "    # Eventually hits token limits!\n",
    "\n",
    "# ‚úÖ Good: Implement sliding window for long conversations\n",
    "MAX_HISTORY = 10  # Keep last 10 messages\n",
    "if len(history) > MAX_HISTORY:\n",
    "    history = history[-MAX_HISTORY:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You Learned\n",
    "\n",
    "‚úÖ **The Problem**: Simple `chat()` doesn't remember previous messages\n",
    "\n",
    "‚úÖ **The Solution**: Use `chat_with_history()` to maintain context\n",
    "\n",
    "‚úÖ **Message Structure**: History is a list of `{role, content}` dictionaries\n",
    "\n",
    "‚úÖ **Building Conversations**: Pass history between turns, update with returned value\n",
    "\n",
    "‚úÖ **When to Use Context**: Building on previous responses vs. independent queries\n",
    "\n",
    "‚úÖ **System Prompts**: Include once on first turn, persists in history\n",
    "\n",
    "‚úÖ **Managing History**: Can inspect, modify, or truncate history manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you can maintain conversations, it's time to supercharge your LLM with **tools**!\n",
    "\n",
    "‚û°Ô∏è Continue to [04-tool-calling-basics.ipynb](./04-tool-calling-basics.ipynb) to learn how to:\n",
    "- Understand what tools are and why they're powerful\n",
    "- Use built-in tools like math_calculator and text_transformer\n",
    "- See how the SDK automatically executes tools\n",
    "- Inspect tool calls in responses\n",
    "- Combine tools with conversation history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
