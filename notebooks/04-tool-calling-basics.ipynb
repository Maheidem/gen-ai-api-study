{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß 04: Tool Calling Basics\n",
    "\n",
    "Learn how to extend your LLM's capabilities by giving it access to built-in tools for calculations, text transformations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìã Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n- [ ] Understand what tools are and why they're useful\n- [ ] Use the unified `bash` tool for command-line operations\n- [ ] Execute Python for math, text processing, and analysis\n- [ ] Use bash commands for file operations and text manipulation\n- [ ] See how tools are automatically executed by the SDK\n- [ ] Inspect tool calls in full responses\n- [ ] Combine multiple bash commands in a single conversation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prerequisites\n",
    "\n",
    "- Completed notebooks 02 (Basic Chat) and 03 (Conversation History)\n",
    "- Understanding of chat and conversation history\n",
    "- LM Studio running with a model that supports function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Estimated Time: 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What Are Tools?\n",
    "\n",
    "**Tools** (also called function calling) let LLMs interact with external functions. This solves key limitations:\n",
    "\n",
    "| Without Tools | With Tools |\n",
    "|---------------|------------|\n",
    "| ‚ùå Bad at precise math | ‚úÖ Use calculator for exact answers |\n",
    "| ‚ùå Can't access real-time data | ‚úÖ Call APIs for current information |\n",
    "| ‚ùå Can't modify files | ‚úÖ Use file operations |\n",
    "| ‚ùå No access to external systems | ‚úÖ Integrate with databases, services |\n",
    "\n",
    "**How it works:**\n",
    "1. You give the LLM a list of available tools\n",
    "2. The LLM decides when to use a tool\n",
    "3. The LLM generates a tool call with parameters\n",
    "4. The SDK executes the tool automatically\n",
    "5. The result goes back to the LLM\n",
    "6. The LLM incorporates it into the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2Ô∏è‚É£ The Unified Bash Tool\n\nThe SDK provides a single powerful `bash` tool that can execute any command-line operation. This replaces specialized tools with a flexible, unified approach.\n\n**What you can do with the bash tool:**\n- Execute Python code for calculations\n- Run shell commands for text processing\n- Perform file operations\n- Use standard Unix utilities (wc, grep, sed, etc.)\n- Chain commands with pipes\n\n**Why a unified tool?**\n- Simpler for LLMs to learn (one tool instead of many)\n- More flexible (any shell command works)\n- Fewer prompt tokens (shorter tool schema)\n- Easier to maintain (one implementation)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Auto-detected model: qwen/qwen3-coder-30b\n",
      "‚úÖ Registered tools:\n",
      "  - bash\n"
     ]
    }
   ],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create client\n",
    "client = LocalLLMClient(\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\"),\n",
    "    model=os.getenv(\"LLM_MODEL\")\n",
    ")\n",
    "\n",
    "# Register built-in tools (pass None to load defaults)\n",
    "client.register_tools_from(None)\n",
    "\n",
    "print(\"‚úÖ Registered tools:\")\n",
    "for tool_name in client.tools.list_tools():\n",
    "    print(f\"  - {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow tracking URI updated: file:///Users/maheidem/Documents/dev/gen-ai-api-study/mlruns\n",
      "\n",
      "üîç Current tracking URI: file:///Users/maheidem/Documents/dev/gen-ai-api-study/mlruns\n",
      "‚úÖ Experiment set: 04-tool-calling-basics\n",
      "\n",
      "üí° Now run your agent tasks and check http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import ipynbname\n",
    "\n",
    "\n",
    "# Set tracking URI to project root (where MLflow UI is serving from)\n",
    "project_root = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "tracking_uri = f\"file://{project_root}/mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"‚úÖ MLflow tracking URI updated: {tracking_uri}\")\n",
    "\n",
    "# Verify it's set correctly\n",
    "print(f\"\\nüîç Current tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Use the notebook filename to name the experiment so runs stay organized\n",
    "experiment_name = f\"{ipynbname.name()}\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"‚úÖ Experiment set: {experiment_name}\")\n",
    "print(\"\\nüí° Now run your agent tasks and check http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Safety Note:**\n",
    "\n",
    "The SDK includes automatic validation to catch model issues:\n",
    "- **XML format errors** (some models use XML instead of JSON)\n",
    "- **Repetition loops** (token generation stuck repeating)\n",
    "\n",
    "By default, validation is **disabled** for development. Learn how to enable it in notebook 05b!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Quick Tip:** You can also use the convenience function to create a client with tools in one step:\n",
    "\n",
    "```python\n",
    "from local_llm_sdk import create_client_with_tools\n",
    "\n",
    "client = create_client_with_tools(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"\n",
    ")\n",
    "# Tools are already registered!\n",
    "```\n",
    "\n",
    "Now let's use the calculator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ask a math question\nresponse = client.chat(\"What is 127 multiplied by 893?\", use_tools=True)\nprint(response)\n\n# Show tool execution details\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**üéâ Behind the scenes:**\n1. The LLM recognized this needs calculation\n2. It called `bash(command=\"python -c 'print(127 * 893)'\")`\n3. The SDK executed the bash command\n4. Python calculated: `127 * 893 = 113411`\n5. The LLM received the result\n6. The LLM formatted a natural language response\n\n**üí° The `print_tool_calls()` method shows:**\n- Which tools were called\n- What commands were executed\n- What results were returned\n- All in a clean, readable format!\n\nTry `client.print_tool_calls(detailed=True)` for full JSON output."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more complex calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complex expression\nresponse = client.chat(\"Calculate: (15 + 25) * 3 / 2\", use_tools=True)\nprint(\"Question: Calculate: (15 + 25) * 3 / 2\")\nprint(f\"Answer: {response}\\n\")\nclient.print_tool_calls()\n\n# With context\nresponse = client.chat(\n    \"If I have 12 boxes with 24 items each, and I sell them at $3.50 per item, \"\n    \"how much revenue do I make?\",\n    use_tools=True\n)\nprint(\"\\nQuestion: Revenue calculation\")\nprint(f\"Answer: {response}\\n\")\nclient.print_tool_calls()\n\n# Multiple calculations\nresponse = client.chat(\n    \"What's the area of a rectangle with width 15.5 and height 23.7? \"\n    \"And what's the perimeter?\",\n    use_tools=True\n)\nprint(\"\\nQuestion: Rectangle area and perimeter\")\nprint(f\"Answer: {response}\\n\")\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3Ô∏è‚É£ Text Processing with Bash\n\nThe bash tool can handle text transformations using Python or shell commands."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uppercase transformation\nresponse = client.chat(\"Convert 'hello world' to uppercase\", use_tools=True, tool_choice=\"required\")\nprint(\"Uppercase:\", response)\nclient.print_tool_calls()\n\n# Lowercase transformation\nresponse = client.chat(\"Make 'PYTHON IS AWESOME' all lowercase\", use_tools=True, tool_choice=\"required\")\nprint(\"\\nLowercase:\", response)\nclient.print_tool_calls()\n\n# Reverse text\nresponse = client.chat(\"Reverse the text: 'artificial intelligence'\", use_tools=True, tool_choice=\"required\")\nprint(\"\\nReverse:\", response)\nclient.print_tool_calls()\n\n# Count words\nresponse = client.chat(\n    \"How many words are in this sentence: 'The quick brown fox jumps over the lazy dog'\",\n    use_tools=True,\n    tool_choice=\"required\"\n)\nprint(\"\\nWord count:\", response)\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4Ô∏è‚É£ Character Counting with Bash\n\nThe bash tool can count characters using Python or shell utilities like `wc`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count with spaces\nresponse = client.chat(\"How many characters are in 'Hello, World!' including spaces?\", use_tools=True, tool_choice=\"required\")\nprint(\"With spaces:\", response)\nclient.print_tool_calls()\n\n# Count without spaces\nresponse = client.chat(\"How many characters are in 'Hello, World!' excluding spaces?\", use_tools=True, tool_choice=\"required\")\nprint(\"\\nWithout spaces:\", response)\nclient.print_tool_calls()\n\n# Compare lengths\nresponse = client.chat(\n    \"Which is longer: 'supercalifragilisticexpialidocious' or 'antidisestablishmentarianism'? \"\n    \"Tell me the character count of each.\",\n    use_tools=True,\n    tool_choice=\"required\"\n)\nprint(\"\\nComparison:\", response)\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Inspecting Tool Calls\n",
    "\n",
    "Two ways to see tool execution details:\n",
    "\n",
    "1. **Quick Summary**: `client.print_tool_calls()` - Clean, readable format\n",
    "2. **Full Details**: `return_full_response=True` - Complete ChatCompletion object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"METHOD 1: Quick summary with print_tool_calls()\")\nprint(\"=\" * 70)\n\nresponse = client.chat(\"What is 456 * 789 and also uppercase the word 'python'\", use_tools=True)\nprint(f\"\\nResponse: {response}\\n\")\n\n# Show compact summary\nclient.print_tool_calls()\n\n# Show detailed version\nprint(\"\\n\\nSame call with detailed=True:\")\nclient.print_tool_calls(detailed=True)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nMETHOD 2: Full ChatCompletion object\")\nprint(\"=\" * 70 + \"\\n\")\n\n# Get full response object\nresponse = client.chat(\n    \"Calculate 15 + 25 and reverse the text 'hello world'\",\n    use_tools=True,\n    return_full_response=True\n)\n\nprint(f\"Model: {response.model}\")\nprint(f\"Finish reason: {response.choices[0].finish_reason}\")\nprint(f\"Final message: {response.choices[0].message.content}\")\n\n# Check tool_calls in the response\nmessage = response.choices[0].message\nif message.tool_calls:\n    print(f\"\\nüîß Tool Calls Made: {len(message.tool_calls)}\")\n    for i, tool_call in enumerate(message.tool_calls, 1):\n        print(f\"\\nTool Call {i}:\")\n        print(f\"  Function: {tool_call.function.name}\")\n        print(f\"  Arguments: {tool_call.function.arguments}\")\n        print(f\"  ID: {tool_call.id}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Which method to use:**\n",
    "\n",
    "**`client.print_tool_calls()`** - Best for:\n",
    "- ‚úÖ Quick debugging\n",
    "- ‚úÖ Learning and demonstrations\n",
    "- ‚úÖ Clean, readable output\n",
    "- ‚úÖ Shows both arguments AND results\n",
    "\n",
    "**`return_full_response=True`** - Best for:\n",
    "- ‚úÖ Programmatic access to tool_calls\n",
    "- ‚úÖ Building automated workflows\n",
    "- ‚úÖ Full metadata (tokens, timing, etc.)\n",
    "- ‚úÖ Inspecting raw ChatCompletion structure\n",
    "\n",
    "**Pro tip:** Use `print_tool_calls(detailed=True)` for full JSON inspection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Combining Multiple Tools\n",
    "\n",
    "The LLM can use multiple tools in a single conversation to solve complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complex request requiring multiple tools\nresponse = client.chat(\n    \"I have a text: 'The Quick BROWN fox'. \"\n    \"Make it all lowercase, count the characters (no spaces), \"\n    \"and if the character count is even, multiply it by 5, otherwise multiply by 3.\",\n    use_tools=True\n)\n\nprint(\"Result:\", response)\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**üß† The LLM orchestrated:**\n1. `bash` to lowercase the text (Python or shell)\n2. `bash` to count characters without spaces\n3. `bash` to multiply based on even/odd condition\n\nThis shows the power of the unified bash tool - one tool, infinite possibilities!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Tools with Conversation History\n",
    "\n",
    "Tools work seamlessly with conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation with tools\n",
    "history = []\n",
    "\n",
    "# Turn 1: Ask about a calculation\n",
    "# NOTE: Some models may skip tools for simple math they can do mentally\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What is 25 * 16?\",\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"Turn 1:\")\n",
    "print(f\"You: What is 25 * 16?\")\n",
    "print(f\"LLM: {response1}\")\n",
    "print()\n",
    "client.print_tool_calls()  # Show if tools were used\n",
    "\n",
    "# Turn 2: Reference previous result  \n",
    "response2, history = client.chat_with_history(\n",
    "    \"Now add 100 to that result.\",\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"\\nTurn 2:\")\n",
    "print(f\"You: Now add 100 to that result.\")\n",
    "print(f\"LLM: {response2}\")\n",
    "print()\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Turn 3: More complex operation\n",
    "response3, history = client.chat_with_history(\n",
    "    \"What is 12847 multiplied by 9283? Be precise.\",  # Changed to force tool use\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"\\nTurn 3:\")\n",
    "print(f\"You: What is 12847 multiplied by 9283? Be precise.\")\n",
    "print(f\"LLM: {response3}\")\n",
    "print()\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Important Model Behavior:**\n",
    "\n",
    "Notice that NO tools were used in any of the turns above! This is because:\n",
    "- **Magistral is a reasoning model** - It thinks through problems in `[THINK]` blocks\n",
    "- During thinking, it solves problems mentally before deciding to use tools\n",
    "- For simple math (25*16, 400+100), it concludes \"I can do this\" ‚Üí no tool call\n",
    "\n",
    "**The Solution: `tool_choice` Parameter**\n",
    "\n",
    "Use `tool_choice=\"required\"` to FORCE tool usage, bypassing internal reasoning:\n",
    "\n",
    "```python\n",
    "# Force tool usage with tool_choice=\"required\"\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"  # Forces the model to use a tool\n",
    ")\n",
    "```\n",
    "\n",
    "Let's see this in action below! üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£b. Forcing Tool Usage with `tool_choice`\n",
    "\n",
    "The `tool_choice` parameter controls whether and when the model uses tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEMO: tool_choice Parameter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: tool_choice=\"auto\" (default - model decides)\n",
    "print(\"\\n1Ô∏è‚É£ tool_choice='auto' (default):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"auto\"  # Model decides\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Test 2: tool_choice=\"required\" (force tool use)\n",
    "print(\"\\n2Ô∏è‚É£ tool_choice='required' (force tool use):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"  # FORCE tool usage\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Test 3: tool_choice=\"none\" (prevent tool use)\n",
    "print(\"\\n3Ô∏è‚É£ tool_choice='none' (prevent tool use):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"none\"  # Prevent tools\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ With tool_choice='required', the model MUST use a tool!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**üìö Understanding `tool_choice` Options:**\n\n| Value | Behavior | Use When |\n|-------|----------|----------|\n| `\"auto\"` | Model decides if tools are needed | Default - balanced approach |\n| `\"required\"` | Forces model to use at least one tool | Need guaranteed tool execution |\n| `\"none\"` | Prevents all tool usage | Want pure LLM reasoning |\n\n**‚ö†Ô∏è Trade-off with Reasoning Models:**\n- `tool_choice=\"auto\"`: Model thinks first, may skip tools for simple tasks\n- `tool_choice=\"required\"`: Bypasses thinking, goes straight to tool usage\n- For **Magistral/reasoning models**: Use `\"required\"` when tools are mandatory\n\n**When to use each:**\n- **\"auto\"**: General use, let the smart model decide\n- **\"required\"**: Calculator apps, API wrappers, guaranteed tool execution  \n- **\"none\"**: Creative writing, brainstorming, pure reasoning tasks\n\n**Advanced:** You can also force a specific tool:\n```python\nresponse = client.chat(\n    \"Calculate something\",\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"bash\"}}\n)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üèãÔ∏è Exercise: Multi-Step Text Analyzer\n\n**Challenge:** Create a conversation that:\n1. Takes a sample text: \"The LOCAL LLM SDK makes AI Development EASIER!\"\n2. Converts it to lowercase\n3. Counts characters with and without spaces\n4. Counts words\n5. Calculates the average character-per-word ratio\n\nRequirements:\n- Use the bash tool for all operations\n- Show each step of the analysis\n- Display the final statistics\n\nTry it yourself first!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Multi-tool text analyzer\n",
    "\n",
    "sample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
    "\n",
    "print(\"üìù Text Analysis Tool Demo\\n\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Analysis request\n",
    "response = client.chat(\n",
    "    f\"Analyze this text: '{sample_text}'. \"\n",
    "    f\"First, convert it to lowercase. \"\n",
    "    f\"Then tell me: \"\n",
    "    f\"(1) how many characters including spaces, \"\n",
    "    f\"(2) how many characters excluding spaces, \"\n",
    "    f\"(3) how many words, and \"\n",
    "    f\"(4) calculate the average characters per word (chars without spaces / word count).\"\n",
    ")\n",
    "\n",
    "print(\"üîç Analysis Results:\")\n",
    "print(response)\n",
    "\n",
    "# Alternative: Step by step with conversation history\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä Step-by-Step Analysis:\\n\")\n",
    "\n",
    "history = []\n",
    "\n",
    "# Step 1: Lowercase\n",
    "response1, history = client.chat_with_history(\n",
    "    f\"Convert to lowercase: '{sample_text}'\",\n",
    "    history\n",
    ")\n",
    "print(f\"Step 1 - Lowercase: {response1}\")\n",
    "\n",
    "# Step 2: Character counts\n",
    "response2, history = client.chat_with_history(\n",
    "    \"Count characters in that lowercased text, both with and without spaces.\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 2 - Char counts: {response2}\")\n",
    "\n",
    "# Step 3: Word count\n",
    "response3, history = client.chat_with_history(\n",
    "    \"How many words are in it?\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 3 - Word count: {response3}\")\n",
    "\n",
    "# Step 4: Average\n",
    "response4, history = client.chat_with_history(\n",
    "    \"Calculate the average characters per word (using chars without spaces).\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 4 - Average: {response4}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution cell (run this to see the answer)\nsample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n\nprint(\"üìù Text Analysis Tool Demo\\n\")\nprint(f\"Original text: {sample_text}\")\nprint(\"=\"*70 + \"\\n\")\n\n# One-shot analysis\nresponse = client.chat(\n    f\"Analyze this text: '{sample_text}'. \"\n    f\"First, convert it to lowercase. \"\n    f\"Then tell me: \"\n    f\"(1) how many characters including spaces, \"\n    f\"(2) how many characters excluding spaces, \"\n    f\"(3) how many words, and \"\n    f\"(4) calculate the average characters per word (chars without spaces / word count).\",\n    use_tools=True\n)\n\nprint(\"üîç Analysis Results:\")\nprint(response)\n\n# Step-by-step version\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nüìä Step-by-Step Analysis:\\n\")\n\nhistory = []\n\nresponse1, history = client.chat_with_history(\n    f\"Convert to lowercase: '{sample_text}'\",\n    history,\n    use_tools=True\n)\nprint(f\"Step 1 - Lowercase: {response1}\")\nclient.print_tool_calls()\n\nresponse2, history = client.chat_with_history(\n    \"Count characters in that lowercased text, both with and without spaces.\",\n    history,\n    use_tools=True\n)\nprint(f\"\\nStep 2 - Char counts: {response2}\")\nclient.print_tool_calls()\n\nresponse3, history = client.chat_with_history(\n    \"How many words are in it?\",\n    history,\n    use_tools=True\n)\nprint(f\"\\nStep 3 - Word count: {response3}\")\nclient.print_tool_calls()\n\nresponse4, history = client.chat_with_history(\n    \"Calculate the average characters per word (using chars without spaces).\",\n    history,\n    use_tools=True\n)\nprint(f\"\\nStep 4 - Average: {response4}\")\nclient.print_tool_calls()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Bonus: Unified MLflow Tracing\n",
    "\n",
    "When you have multiple related chat calls, you can group them under a single parent trace using `client.conversation()`. This creates a clean hierarchy in MLflow instead of multiple separate traces.\n",
    "\n",
    "**Without grouping:**\n",
    "- Each `client.chat()` call creates a separate top-level trace\n",
    "- Hard to see relationships between calls\n",
    "- MLflow UI gets cluttered\n",
    "\n",
    "**With grouping:**\n",
    "- All calls nested under one parent trace\n",
    "- Clear workflow hierarchy\n",
    "- Easy to analyze the complete interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"üìä Grouped Tracing Demo\")\nprint(\"=\" * 70)\n\n# Solution cell (run this to see the answer)\nsample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n\nprint(\"üìù Text Analysis Tool Demo\\n\")\nprint(f\"Original text: {sample_text}\")\nprint(\"=\"*70 + \"\\n\")\n\n# Use conversation context to group all calls\nwith client.conversation(\"text_analysis_grouped\"):\n    print(\"\\nüîÑ Running grouped analysis...\\n\")\n    \n    # One-shot analysis (becomes child trace)\n    response = client.chat(\n        f\"Analyze this text: '{sample_text}'. \"\n        f\"Convert to lowercase and count: \"\n        f\"(1) characters with spaces, \"\n        f\"(2) characters without spaces, \"\n        f\"(3) words.\",\n        use_tools=True\n    )\n    print(\"Quick analysis:\", response)\n    \n    # Step-by-step (all become child traces of the same parent)\n    history = []\n    \n    response1, history = client.chat_with_history(\n        f\"What's uppercase of 'sdk'?\",\n        history,\n        use_tools=True\n    )\n    print(f\"\\nStep 1: {response1}\")\n    \n    response2, history = client.chat_with_history(\n        \"How many characters in that uppercase result?\",\n        history,\n        use_tools=True\n    )\n    print(f\"Step 2: {response2}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úÖ Check MLflow UI: All calls grouped under 'text_analysis_grouped'\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ What Just Happened:**\n",
    "\n",
    "All chat calls inside `with client.conversation(\"name\"):` are grouped as children of a parent trace.\n",
    "\n",
    "**MLflow Hierarchy:**\n",
    "```\n",
    "text_analysis_grouped (parent)\n",
    "‚îú‚îÄ chat (quick analysis)\n",
    "‚îÇ  ‚îú‚îÄ send_request\n",
    "‚îÇ  ‚îî‚îÄ handle_tool_calls\n",
    "‚îú‚îÄ chat (step 1)\n",
    "‚îÇ  ‚îú‚îÄ send_request\n",
    "‚îÇ  ‚îî‚îÄ handle_tool_calls\n",
    "‚îî‚îÄ chat (step 2)\n",
    "   ‚îú‚îÄ send_request\n",
    "   ‚îî‚îÄ handle_tool_calls\n",
    "```\n",
    "\n",
    "**üí° When to use:**\n",
    "- Multi-step workflows\n",
    "- Agent iterations (agents use this internally!)\n",
    "- Related chat calls that form a logical unit\n",
    "- Debugging complex interactions\n",
    "\n",
    "**Pro tip:** This is exactly what the ReACT agent uses internally to group all its iterations! You'll see this pattern in notebook 07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Register Tools\n",
    "```python\n",
    "# ‚ùå Bad: Tools not registered\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "response = client.chat(\"Calculate 5 * 5\")\n",
    "# LLM tries to do math itself (often wrong)\n",
    "\n",
    "# ‚úÖ Good: Register tools first\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "client.register_tools_from(None)  # Load built-in tools\n",
    "response = client.chat(\"Calculate 5 * 5\")\n",
    "```\n",
    "\n",
    "### 2. Not Enabling Tools in Chat Call\n",
    "```python\n",
    "# ‚ùå Bad: Tools registered but not used\n",
    "client.register_tools_from(None)\n",
    "response = client.chat(\"Calculate 5 * 5\")  # Tools available but not used\n",
    "\n",
    "# ‚úÖ Good: Enable tools in chat (default is use_tools=True)\n",
    "response = client.chat(\"Calculate 5 * 5\", use_tools=True)\n",
    "# Or just: client.chat(\"Calculate 5 * 5\") since use_tools=True is default\n",
    "```\n",
    "\n",
    "### 3. Model Doesn't Support Function Calling\n",
    "```python\n",
    "# ‚ö†Ô∏è Some models don't support function calling well\n",
    "# Check your model's capabilities:\n",
    "# - Qwen, Hermes, Functionary, Mistral: Good function calling\n",
    "# - Older or smaller models: May not support it\n",
    "\n",
    "# Test with a simple tool call to verify\n",
    "response = client.chat(\"Calculate 123 * 456\", use_tools=True)\n",
    "# If answer is approximated or wrong, model may not support tools\n",
    "```\n",
    "\n",
    "### 4. Expecting Tools for Simple Tasks\n",
    "```python\n",
    "# ‚ö†Ô∏è For very simple math, LLM might not use tools\n",
    "response = client.chat(\"What is 2 + 2?\")\n",
    "# LLM knows this and may answer directly: \"4\"\n",
    "\n",
    "# Tools are used for:\n",
    "# - Complex calculations: \"What is 12847 * 9283?\"\n",
    "# - Precise operations: \"Calculate exactly: 15.7 / 3.2\"\n",
    "# - Multi-step tasks: \"Calculate (5+3) * (10-2) / 4\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéì What You Learned\n\n‚úÖ **Tool Concept**: Functions that extend LLM capabilities beyond text generation\n\n‚úÖ **Unified Bash Tool**: One powerful tool for calculations, text processing, file operations, and more\n\n‚úÖ **Automatic Execution**: SDK handles tool calls transparently when `use_tools=True`\n\n‚úÖ **Tool Registration**: Use `client.register_tools_from(None)` to load built-in tools\n\n‚úÖ **Tool Inspection**: Use `client.print_tool_calls()` or `return_full_response=True` to see details\n\n‚úÖ **Multi-Step Tasks**: LLM can orchestrate multiple bash commands for complex operations\n\n‚úÖ **Tools + History**: Combine tools with conversation context for powerful workflows"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "You've mastered built-in tools! Now let's create your own custom tools.\n",
    "\n",
    "‚û°Ô∏è Continue to [05-custom-tools.ipynb](./05-custom-tools.ipynb) to learn how to:\n",
    "- Create custom tools with the `@tool` decorator\n",
    "- Define parameters with type hints\n",
    "- Handle errors in tool functions\n",
    "- Register and use your own tools\n",
    "- Build a complete unit converter tool\n",
    "- Follow best practices for tool design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}