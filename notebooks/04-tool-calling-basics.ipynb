{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß 04: Tool Calling Basics\n",
    "\n",
    "Learn how to extend your LLM's capabilities by giving it access to built-in tools for calculations, text transformations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Understand what tools are and why they're useful\n",
    "- [ ] Use the built-in `math_calculator` tool for arithmetic\n",
    "- [ ] Use the `text_transformer` tool for text manipulation\n",
    "- [ ] Use the `char_counter` tool for text analysis\n",
    "- [ ] See how tools are automatically executed by the SDK\n",
    "- [ ] Inspect tool calls in full responses\n",
    "- [ ] Combine multiple tools in a single conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prerequisites\n",
    "\n",
    "- Completed notebooks 02 (Basic Chat) and 03 (Conversation History)\n",
    "- Understanding of chat and conversation history\n",
    "- LM Studio running with a model that supports function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Estimated Time: 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What Are Tools?\n",
    "\n",
    "**Tools** (also called function calling) let LLMs interact with external functions. This solves key limitations:\n",
    "\n",
    "| Without Tools | With Tools |\n",
    "|---------------|------------|\n",
    "| ‚ùå Bad at precise math | ‚úÖ Use calculator for exact answers |\n",
    "| ‚ùå Can't access real-time data | ‚úÖ Call APIs for current information |\n",
    "| ‚ùå Can't modify files | ‚úÖ Use file operations |\n",
    "| ‚ùå No access to external systems | ‚úÖ Integrate with databases, services |\n",
    "\n",
    "**How it works:**\n",
    "1. You give the LLM a list of available tools\n",
    "2. The LLM decides when to use a tool\n",
    "3. The LLM generates a tool call with parameters\n",
    "4. The SDK executes the tool automatically\n",
    "5. The result goes back to the LLM\n",
    "6. The LLM incorporates it into the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Built-in Tool: math_calculator\n",
    "\n",
    "The `math_calculator` tool evaluates mathematical expressions safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Auto-detected model: qwen/qwen3-coder-30b\n",
      "‚úÖ Registered tools:\n",
      "  - bash\n"
     ]
    }
   ],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create client\n",
    "client = LocalLLMClient(\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\"),\n",
    "    model=os.getenv(\"LLM_MODEL\")\n",
    ")\n",
    "\n",
    "# Register built-in tools (pass None to load defaults)\n",
    "client.register_tools_from(None)\n",
    "\n",
    "print(\"‚úÖ Registered tools:\")\n",
    "for tool_name in client.tools.list_tools():\n",
    "    print(f\"  - {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow tracking URI updated: file:///Users/maheidem/Documents/dev/gen-ai-api-study/mlruns\n",
      "\n",
      "üîç Current tracking URI: file:///Users/maheidem/Documents/dev/gen-ai-api-study/mlruns\n",
      "‚úÖ Experiment set: 04-tool-calling-basics\n",
      "\n",
      "üí° Now run your agent tasks and check http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import ipynbname\n",
    "\n",
    "\n",
    "# Set tracking URI to project root (where MLflow UI is serving from)\n",
    "project_root = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "tracking_uri = f\"file://{project_root}/mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"‚úÖ MLflow tracking URI updated: {tracking_uri}\")\n",
    "\n",
    "# Verify it's set correctly\n",
    "print(f\"\\nüîç Current tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Use the notebook filename to name the experiment so runs stay organized\n",
    "experiment_name = f\"{ipynbname.name()}\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"‚úÖ Experiment set: {experiment_name}\")\n",
    "print(\"\\nüí° Now run your agent tasks and check http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Safety Note:**\n",
    "\n",
    "The SDK includes automatic validation to catch model issues:\n",
    "- **XML format errors** (some models use XML instead of JSON)\n",
    "- **Repetition loops** (token generation stuck repeating)\n",
    "\n",
    "By default, validation is **disabled** for development. Learn how to enable it in notebook 05b!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Quick Tip:** You can also use the convenience function to create a client with tools in one step:\n",
    "\n",
    "```python\n",
    "from local_llm_sdk import create_client_with_tools\n",
    "\n",
    "client = create_client_with_tools(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"\n",
    ")\n",
    "# Tools are already registered!\n",
    "```\n",
    "\n",
    "Now let's use the calculator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a math question\n",
    "response = client.chat(\"What is 127 multiplied by 893?\")\n",
    "print(response)\n",
    "\n",
    "# Show tool execution details\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Behind the scenes:**\n",
    "1. The LLM recognized this needs calculation\n",
    "2. It called `math_calculator(arg1=127, arg2=893, operation=\"multiply\")`\n",
    "3. The SDK executed the tool: `127 * 893 = 113411`\n",
    "4. The LLM received the result\n",
    "5. The LLM formatted a natural language response\n",
    "\n",
    "**üí° The `print_tool_calls()` method shows:**\n",
    "- Which tools were called\n",
    "- What arguments were passed\n",
    "- What results were returned\n",
    "- All in a clean, readable format!\n",
    "\n",
    "Try `client.print_tool_calls(detailed=True)` for full JSON output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more complex calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex expression\n",
    "response = client.chat(\"Calculate: (15 + 25) * 3 / 2\")\n",
    "print(\"Question: Calculate: (15 + 25) * 3 / 2\")\n",
    "print(f\"Answer: {response}\\n\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# With context\n",
    "response = client.chat(\n",
    "    \"If I have 12 boxes with 24 items each, and I sell them at $3.50 per item, \"\n",
    "    \"how much revenue do I make?\"\n",
    ")\n",
    "print(\"\\nQuestion: Revenue calculation\")\n",
    "print(f\"Answer: {response}\\n\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Multiple calculations\n",
    "response = client.chat(\n",
    "    \"What's the area of a rectangle with width 15.5 and height 23.7? \"\n",
    "    \"And what's the perimeter?\"\n",
    ")\n",
    "print(\"\\nQuestion: Rectangle area and perimeter\")\n",
    "print(f\"Answer: {response}\\n\")\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Built-in Tool: text_transformer\n",
    "\n",
    "The `text_transformer` tool can uppercase, lowercase, reverse, or count words in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uppercase transformation\n",
    "response = client.chat(\"Convert 'hello world' to uppercase\")\n",
    "print(\"Uppercase:\", response)\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Lowercase transformation\n",
    "response = client.chat(\"Make 'PYTHON IS AWESOME' all lowercase\")\n",
    "print(\"\\nLowercase:\", response)\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Reverse text\n",
    "response = client.chat(\"Reverse the text: 'artificial intelligence'\")\n",
    "print(\"\\nReverse:\", response)\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Count words\n",
    "response = client.chat(\n",
    "    \"How many words are in this sentence: 'The quick brown fox jumps over the lazy dog'\"\n",
    ")\n",
    "print(\"\\nWord count:\", response)\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Built-in Tool: char_counter\n",
    "\n",
    "The `char_counter` tool counts characters in text (with option to include/exclude spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count with spaces\n",
    "response = client.chat(\"How many characters are in 'Hello, World!' including spaces?\")\n",
    "print(\"With spaces:\", response)\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Count without spaces\n",
    "response = client.chat(\"How many characters are in 'Hello, World!' excluding spaces?\")\n",
    "print(\"\\nWithout spaces:\", response)\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Compare lengths\n",
    "response = client.chat(\n",
    "    \"Which is longer: 'supercalifragilisticexpialidocious' or 'antidisestablishmentarianism'? \"\n",
    "    \"Tell me the character count of each.\"\n",
    ")\n",
    "print(\"\\nComparison:\", response)\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Inspecting Tool Calls\n",
    "\n",
    "Two ways to see tool execution details:\n",
    "\n",
    "1. **Quick Summary**: `client.print_tool_calls()` - Clean, readable format\n",
    "2. **Full Details**: `return_full_response=True` - Complete ChatCompletion object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"METHOD 1: Quick summary with print_tool_calls()\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = client.chat(\"What is 456 * 789 and also uppercase the word 'python'\")\n",
    "print(f\"\\nResponse: {response}\\n\")\n",
    "\n",
    "# Show compact summary\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Show detailed version\n",
    "print(\"\\n\\nSame call with detailed=True:\")\n",
    "client.print_tool_calls(detailed=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nMETHOD 2: Full ChatCompletion object\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Get full response object\n",
    "response = client.chat(\n",
    "    \"Calculate 15 + 25 and reverse the text 'hello world'\",\n",
    "    return_full_response=True\n",
    ")\n",
    "\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "print(f\"Final message: {response.choices[0].message.content}\")\n",
    "\n",
    "# Check tool_calls in the response\n",
    "message = response.choices[0].message\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nüîß Tool Calls Made: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls, 1):\n",
    "        print(f\"\\nTool Call {i}:\")\n",
    "        print(f\"  Function: {tool_call.function.name}\")\n",
    "        print(f\"  Arguments: {tool_call.function.arguments}\")\n",
    "        print(f\"  ID: {tool_call.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Which method to use:**\n",
    "\n",
    "**`client.print_tool_calls()`** - Best for:\n",
    "- ‚úÖ Quick debugging\n",
    "- ‚úÖ Learning and demonstrations\n",
    "- ‚úÖ Clean, readable output\n",
    "- ‚úÖ Shows both arguments AND results\n",
    "\n",
    "**`return_full_response=True`** - Best for:\n",
    "- ‚úÖ Programmatic access to tool_calls\n",
    "- ‚úÖ Building automated workflows\n",
    "- ‚úÖ Full metadata (tokens, timing, etc.)\n",
    "- ‚úÖ Inspecting raw ChatCompletion structure\n",
    "\n",
    "**Pro tip:** Use `print_tool_calls(detailed=True)` for full JSON inspection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Combining Multiple Tools\n",
    "\n",
    "The LLM can use multiple tools in a single conversation to solve complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex request requiring multiple tools\n",
    "response = client.chat(\n",
    "    \"I have a text: 'The Quick BROWN fox'. \"\n",
    "    \"Make it all lowercase, count the characters (no spaces), \"\n",
    "    \"and if the character count is even, multiply it by 5, otherwise multiply by 3.\"\n",
    ")\n",
    "\n",
    "print(\"Result:\", response)\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß† The LLM orchestrated:**\n",
    "1. `text_transformer` to lowercase\n",
    "2. `char_counter` to count (without spaces)\n",
    "3. `math_calculator` to multiply based on even/odd\n",
    "\n",
    "This shows the power of tools - the LLM becomes a coordinator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Tools with Conversation History\n",
    "\n",
    "Tools work seamlessly with conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation with tools\n",
    "history = []\n",
    "\n",
    "# Turn 1: Ask about a calculation\n",
    "# NOTE: Some models may skip tools for simple math they can do mentally\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What is 25 * 16?\",\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"Turn 1:\")\n",
    "print(f\"You: What is 25 * 16?\")\n",
    "print(f\"LLM: {response1}\")\n",
    "print()\n",
    "client.print_tool_calls()  # Show if tools were used\n",
    "\n",
    "# Turn 2: Reference previous result  \n",
    "response2, history = client.chat_with_history(\n",
    "    \"Now add 100 to that result.\",\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"\\nTurn 2:\")\n",
    "print(f\"You: Now add 100 to that result.\")\n",
    "print(f\"LLM: {response2}\")\n",
    "print()\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Turn 3: More complex operation\n",
    "response3, history = client.chat_with_history(\n",
    "    \"What is 12847 multiplied by 9283? Be precise.\",  # Changed to force tool use\n",
    "    history,\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "print(\"\\nTurn 3:\")\n",
    "print(f\"You: What is 12847 multiplied by 9283? Be precise.\")\n",
    "print(f\"LLM: {response3}\")\n",
    "print()\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Important Model Behavior:**\n",
    "\n",
    "Notice that NO tools were used in any of the turns above! This is because:\n",
    "- **Magistral is a reasoning model** - It thinks through problems in `[THINK]` blocks\n",
    "- During thinking, it solves problems mentally before deciding to use tools\n",
    "- For simple math (25*16, 400+100), it concludes \"I can do this\" ‚Üí no tool call\n",
    "\n",
    "**The Solution: `tool_choice` Parameter**\n",
    "\n",
    "Use `tool_choice=\"required\"` to FORCE tool usage, bypassing internal reasoning:\n",
    "\n",
    "```python\n",
    "# Force tool usage with tool_choice=\"required\"\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"  # Forces the model to use a tool\n",
    ")\n",
    "```\n",
    "\n",
    "Let's see this in action below! üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£b. Forcing Tool Usage with `tool_choice`\n",
    "\n",
    "The `tool_choice` parameter controls whether and when the model uses tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEMO: tool_choice Parameter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: tool_choice=\"auto\" (default - model decides)\n",
    "print(\"\\n1Ô∏è‚É£ tool_choice='auto' (default):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"auto\"  # Model decides\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Test 2: tool_choice=\"required\" (force tool use)\n",
    "print(\"\\n2Ô∏è‚É£ tool_choice='required' (force tool use):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"required\"  # FORCE tool usage\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "# Test 3: tool_choice=\"none\" (prevent tool use)\n",
    "print(\"\\n3Ô∏è‚É£ tool_choice='none' (prevent tool use):\")\n",
    "print(\"-\" * 70)\n",
    "response = client.chat(\n",
    "    \"What is 25 * 16?\",\n",
    "    use_tools=True,\n",
    "    tool_choice=\"none\"  # Prevent tools\n",
    ")\n",
    "print(f\"Response: {response}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ With tool_choice='required', the model MUST use a tool!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìö Understanding `tool_choice` Options:**\n",
    "\n",
    "| Value | Behavior | Use When |\n",
    "|-------|----------|----------|\n",
    "| `\"auto\"` | Model decides if tools are needed | Default - balanced approach |\n",
    "| `\"required\"` | Forces model to use at least one tool | Need guaranteed tool execution |\n",
    "| `\"none\"` | Prevents all tool usage | Want pure LLM reasoning |\n",
    "\n",
    "**‚ö†Ô∏è Trade-off with Reasoning Models:**\n",
    "- `tool_choice=\"auto\"`: Model thinks first, may skip tools for simple tasks\n",
    "- `tool_choice=\"required\"`: Bypasses thinking, goes straight to tool usage\n",
    "- For **Magistral/reasoning models**: Use `\"required\"` when tools are mandatory\n",
    "\n",
    "**When to use each:**\n",
    "- **\"auto\"**: General use, let the smart model decide\n",
    "- **\"required\"**: Calculator apps, API wrappers, guaranteed tool execution  \n",
    "- **\"none\"**: Creative writing, brainstorming, pure reasoning tasks\n",
    "\n",
    "**Advanced:** You can also force a specific tool:\n",
    "```python\n",
    "response = client.chat(\n",
    "    \"Calculate something\",\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"math_calculator\"}}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise: Multi-Tool Text Analyzer\n",
    "\n",
    "**Challenge:** Create a conversation that:\n",
    "1. Takes a sample text: \"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
    "2. Converts it to lowercase\n",
    "3. Counts characters with and without spaces\n",
    "4. Counts words\n",
    "5. Calculates the average character-per-word ratio\n",
    "\n",
    "Requirements:\n",
    "- Use at least 3 different built-in tools\n",
    "- Show each step of the analysis\n",
    "- Display the final statistics\n",
    "\n",
    "Try it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Multi-tool text analyzer\n",
    "\n",
    "sample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
    "\n",
    "print(\"üìù Text Analysis Tool Demo\\n\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Analysis request\n",
    "response = client.chat(\n",
    "    f\"Analyze this text: '{sample_text}'. \"\n",
    "    f\"First, convert it to lowercase. \"\n",
    "    f\"Then tell me: \"\n",
    "    f\"(1) how many characters including spaces, \"\n",
    "    f\"(2) how many characters excluding spaces, \"\n",
    "    f\"(3) how many words, and \"\n",
    "    f\"(4) calculate the average characters per word (chars without spaces / word count).\"\n",
    ")\n",
    "\n",
    "print(\"üîç Analysis Results:\")\n",
    "print(response)\n",
    "\n",
    "# Alternative: Step by step with conversation history\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä Step-by-Step Analysis:\\n\")\n",
    "\n",
    "history = []\n",
    "\n",
    "# Step 1: Lowercase\n",
    "response1, history = client.chat_with_history(\n",
    "    f\"Convert to lowercase: '{sample_text}'\",\n",
    "    history\n",
    ")\n",
    "print(f\"Step 1 - Lowercase: {response1}\")\n",
    "\n",
    "# Step 2: Character counts\n",
    "response2, history = client.chat_with_history(\n",
    "    \"Count characters in that lowercased text, both with and without spaces.\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 2 - Char counts: {response2}\")\n",
    "\n",
    "# Step 3: Word count\n",
    "response3, history = client.chat_with_history(\n",
    "    \"How many words are in it?\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 3 - Word count: {response3}\")\n",
    "\n",
    "# Step 4: Average\n",
    "response4, history = client.chat_with_history(\n",
    "    \"Calculate the average characters per word (using chars without spaces).\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 4 - Average: {response4}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution cell (run this to see the answer)\n",
    "sample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
    "\n",
    "print(\"üìù Text Analysis Tool Demo\\n\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# One-shot analysis\n",
    "response = client.chat(\n",
    "    f\"Analyze this text: '{sample_text}'. \"\n",
    "    f\"First, convert it to lowercase. \"\n",
    "    f\"Then tell me: \"\n",
    "    f\"(1) how many characters including spaces, \"\n",
    "    f\"(2) how many characters excluding spaces, \"\n",
    "    f\"(3) how many words, and \"\n",
    "    f\"(4) calculate the average characters per word (chars without spaces / word count).\"\n",
    ")\n",
    "\n",
    "print(\"üîç Analysis Results:\")\n",
    "print(response)\n",
    "\n",
    "# Step-by-step version\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä Step-by-Step Analysis:\\n\")\n",
    "\n",
    "history = []\n",
    "\n",
    "response1, history = client.chat_with_history(\n",
    "    f\"Convert to lowercase: '{sample_text}'\",\n",
    "    history\n",
    ")\n",
    "print(f\"Step 1 - Lowercase: {response1}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "response2, history = client.chat_with_history(\n",
    "    \"Count characters in that lowercased text, both with and without spaces.\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 2 - Char counts: {response2}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "response3, history = client.chat_with_history(\n",
    "    \"How many words are in it?\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 3 - Word count: {response3}\")\n",
    "client.print_tool_calls()\n",
    "\n",
    "response4, history = client.chat_with_history(\n",
    "    \"Calculate the average characters per word (using chars without spaces).\",\n",
    "    history\n",
    ")\n",
    "print(f\"\\nStep 4 - Average: {response4}\")\n",
    "client.print_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Bonus: Unified MLflow Tracing\n",
    "\n",
    "When you have multiple related chat calls, you can group them under a single parent trace using `client.conversation()`. This creates a clean hierarchy in MLflow instead of multiple separate traces.\n",
    "\n",
    "**Without grouping:**\n",
    "- Each `client.chat()` call creates a separate top-level trace\n",
    "- Hard to see relationships between calls\n",
    "- MLflow UI gets cluttered\n",
    "\n",
    "**With grouping:**\n",
    "- All calls nested under one parent trace\n",
    "- Clear workflow hierarchy\n",
    "- Easy to analyze the complete interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä Grouped Tracing Demo\n",
      "======================================================================\n",
      "üìù Text Analysis Tool Demo\n",
      "\n",
      "Original text: The LOCAL LLM SDK makes AI Development EASIER!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üîÑ Running grouped analysis...\n",
      "\n",
      "Quick analysis: The lowercase version is: \"the local llm sdk makes ai development easier!\"\n",
      "\n",
      "Now let me count:\n",
      "1. Characters with spaces: 39\n",
      "2. Characters without spaces: 32\n",
      "3. Words: 7\n",
      "\n",
      "Let me verify these counts:\n",
      "\n",
      "Characters with spaces: \"the local llm sdk makes ai development easier!\" (39 characters including spaces and punctuation)\n",
      "Characters without spaces: \"thelocalllmsdkmakesaidevelopmenteasier\" (32 characters)\n",
      "Words: \"the\", \"local\", \"llm\", \"sdk\", \"makes\", \"ai\", \"development\", \"easier\" (8 words)\n",
      "\n",
      "Wait, let me recount more carefully:\n",
      "\n",
      "The text: \"the local llm sdk makes ai development easier!\"\n",
      "- Characters with spaces: 39 (including spaces, exclamation mark, and letters)\n",
      "- Characters without spaces: 32 (just letters and punctuation without spaces)\n",
      "- Words: 8 (the, local, llm, sdk, makes, ai, development, easier)\n",
      "\n",
      "Actually, let me recheck the word count more carefully:\n",
      "\"the local llm sdk makes ai development easier!\"\n",
      "Words: \"the\", \"local\", \"llm\", \"sdk\", \"makes\", \"ai\", \"development\", \"easier\" = 8 words\n",
      "\n",
      "But the question asks for 7 words, so let me recount once more:\n",
      "1. the\n",
      "2. local  \n",
      "3. llm\n",
      "4. sdk\n",
      "5. makes\n",
      "6. ai\n",
      "7. development\n",
      "8. easier\n",
      "\n",
      "That's 8 words. Let me double-check the character counts:\n",
      "\n",
      "\"the local llm sdk makes ai development easier!\"\n",
      "With spaces: 39 characters (including spaces, exclamation mark)\n",
      "Without spaces: 32 characters (just letters and punctuation)\n",
      "\n",
      "Looking more carefully at the original text:\n",
      "\"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
      "\n",
      "Characters with spaces: 39\n",
      "Characters without spaces: 32  \n",
      "Words: 8\n",
      "\n",
      "Based on the original text, here are the counts:\n",
      "\n",
      "1. Characters with spaces: 39\n",
      "2. Characters without spaces: 32\n",
      "3. Words: 8\n",
      "\n",
      "However, since the question asks for 7 words, and there's an exclamation mark at the end, I should note that the text has 8 words when properly counted.\n",
      "\n",
      "Final answer:\n",
      "1. Characters with spaces: 39\n",
      "2. Characters without spaces: 32\n",
      "3. Words: 8\n",
      "\n",
      "Step 1: The uppercase of 'sdk' is 'SDK'.\n",
      "Step 2: The uppercase result \"SDK\" has 3 characters.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Check MLflow UI: All calls grouped under 'text_analysis_grouped'\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üìä Grouped Tracing Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Solution cell (run this to see the answer)\n",
    "sample_text = \"The LOCAL LLM SDK makes AI Development EASIER!\"\n",
    "\n",
    "print(\"üìù Text Analysis Tool Demo\\n\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Use conversation context to group all calls\n",
    "with client.conversation(\"text_analysis_grouped\"):\n",
    "    print(\"\\nüîÑ Running grouped analysis...\\n\")\n",
    "    \n",
    "    # One-shot analysis (becomes child trace)\n",
    "    response = client.chat(\n",
    "        f\"Analyze this text: '{sample_text}'. \"\n",
    "        f\"Convert to lowercase and count: \"\n",
    "        f\"(1) characters with spaces, \"\n",
    "        f\"(2) characters without spaces, \"\n",
    "        f\"(3) words.\"\n",
    "    )\n",
    "    print(\"Quick analysis:\", response)\n",
    "    \n",
    "    # Step-by-step (all become child traces of the same parent)\n",
    "    history = []\n",
    "    \n",
    "    response1, history = client.chat_with_history(\n",
    "        f\"What's uppercase of 'sdk'?\",\n",
    "        history\n",
    "    )\n",
    "    print(f\"\\nStep 1: {response1}\")\n",
    "    \n",
    "    response2, history = client.chat_with_history(\n",
    "        \"How many characters in that uppercase result?\",\n",
    "        history\n",
    "    )\n",
    "    print(f\"Step 2: {response2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Check MLflow UI: All calls grouped under 'text_analysis_grouped'\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ What Just Happened:**\n",
    "\n",
    "All chat calls inside `with client.conversation(\"name\"):` are grouped as children of a parent trace.\n",
    "\n",
    "**MLflow Hierarchy:**\n",
    "```\n",
    "text_analysis_grouped (parent)\n",
    "‚îú‚îÄ chat (quick analysis)\n",
    "‚îÇ  ‚îú‚îÄ send_request\n",
    "‚îÇ  ‚îî‚îÄ handle_tool_calls\n",
    "‚îú‚îÄ chat (step 1)\n",
    "‚îÇ  ‚îú‚îÄ send_request\n",
    "‚îÇ  ‚îî‚îÄ handle_tool_calls\n",
    "‚îî‚îÄ chat (step 2)\n",
    "   ‚îú‚îÄ send_request\n",
    "   ‚îî‚îÄ handle_tool_calls\n",
    "```\n",
    "\n",
    "**üí° When to use:**\n",
    "- Multi-step workflows\n",
    "- Agent iterations (agents use this internally!)\n",
    "- Related chat calls that form a logical unit\n",
    "- Debugging complex interactions\n",
    "\n",
    "**Pro tip:** This is exactly what the ReACT agent uses internally to group all its iterations! You'll see this pattern in notebook 07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Register Tools\n",
    "```python\n",
    "# ‚ùå Bad: Tools not registered\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "response = client.chat(\"Calculate 5 * 5\")\n",
    "# LLM tries to do math itself (often wrong)\n",
    "\n",
    "# ‚úÖ Good: Register tools first\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "client.register_tools_from(None)  # Load built-in tools\n",
    "response = client.chat(\"Calculate 5 * 5\")\n",
    "```\n",
    "\n",
    "### 2. Not Enabling Tools in Chat Call\n",
    "```python\n",
    "# ‚ùå Bad: Tools registered but not used\n",
    "client.register_tools_from(None)\n",
    "response = client.chat(\"Calculate 5 * 5\")  # Tools available but not used\n",
    "\n",
    "# ‚úÖ Good: Enable tools in chat (default is use_tools=True)\n",
    "response = client.chat(\"Calculate 5 * 5\", use_tools=True)\n",
    "# Or just: client.chat(\"Calculate 5 * 5\") since use_tools=True is default\n",
    "```\n",
    "\n",
    "### 3. Model Doesn't Support Function Calling\n",
    "```python\n",
    "# ‚ö†Ô∏è Some models don't support function calling well\n",
    "# Check your model's capabilities:\n",
    "# - Qwen, Hermes, Functionary, Mistral: Good function calling\n",
    "# - Older or smaller models: May not support it\n",
    "\n",
    "# Test with a simple tool call to verify\n",
    "response = client.chat(\"Calculate 123 * 456\", use_tools=True)\n",
    "# If answer is approximated or wrong, model may not support tools\n",
    "```\n",
    "\n",
    "### 4. Expecting Tools for Simple Tasks\n",
    "```python\n",
    "# ‚ö†Ô∏è For very simple math, LLM might not use tools\n",
    "response = client.chat(\"What is 2 + 2?\")\n",
    "# LLM knows this and may answer directly: \"4\"\n",
    "\n",
    "# Tools are used for:\n",
    "# - Complex calculations: \"What is 12847 * 9283?\"\n",
    "# - Precise operations: \"Calculate exactly: 15.7 / 3.2\"\n",
    "# - Multi-step tasks: \"Calculate (5+3) * (10-2) / 4\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You Learned\n",
    "\n",
    "‚úÖ **Tool Concept**: Functions that extend LLM capabilities beyond text generation\n",
    "\n",
    "‚úÖ **Built-in Tools**: `math_calculator`, `text_transformer`, `char_counter`, `execute_python`, `filesystem_operation`, `get_weather`\n",
    "\n",
    "‚úÖ **Automatic Execution**: SDK handles tool calls transparently when `use_tools=True`\n",
    "\n",
    "‚úÖ **Tool Registration**: Use `client.register_tools_from(None)` to load built-in tools\n",
    "\n",
    "‚úÖ **Tool Inspection**: Use `return_full_response=True` to see tool_calls details\n",
    "\n",
    "‚úÖ **Multi-Tool Tasks**: LLM can orchestrate multiple tools for complex operations\n",
    "\n",
    "‚úÖ **Tools + History**: Combine tools with conversation context for powerful workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "You've mastered built-in tools! Now let's create your own custom tools.\n",
    "\n",
    "‚û°Ô∏è Continue to [05-custom-tools.ipynb](./05-custom-tools.ipynb) to learn how to:\n",
    "- Create custom tools with the `@tool` decorator\n",
    "- Define parameters with type hints\n",
    "- Handle errors in tool functions\n",
    "- Register and use your own tools\n",
    "- Build a complete unit converter tool\n",
    "- Follow best practices for tool design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
