{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM SDK - Hello World\n",
    "\n",
    "This notebook demonstrates how to use the `local_llm_sdk` package to interact with LM Studio and other OpenAI-compatible local LLM servers.\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio running locally with API server enabled\n",
    "- Install the package: `pip install -e ..` (from notebooks directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/maheidem/gen-ai-api-study\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic>=2.0.0 (from local-llm-sdk==0.1.0)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting requests>=2.28.0 (from local-llm-sdk==0.1.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->local-llm-sdk==0.1.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.0.0->local-llm-sdk==0.1.0)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.0.0->local-llm-sdk==0.1.0)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.0.0->local-llm-sdk==0.1.0)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.28.0->local-llm-sdk==0.1.0)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.28.0->local-llm-sdk==0.1.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.28.0->local-llm-sdk==0.1.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.28.0->local-llm-sdk==0.1.0)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, idna, charset_normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, pydantic, local-llm-sdk\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0\n",
      "\u001b[2K  Attempting uninstall: idna\n",
      "\u001b[2K    Found existing installation: idna 3.10\n",
      "\u001b[2K    Uninstalling idna-3.10:\n",
      "\u001b[2K      Successfully uninstalled idna-3.10\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: certifi━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: annotated-types━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: annotated-types 0.7.0━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling annotated-types-0.7.0:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled annotated-types-0.7.0━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: typing-inspection━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: typing-inspection 0.4.1━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling typing-inspection-0.4.1:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled typing-inspection-0.4.1━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: requestsm\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Found existing installation: requests 2.32.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K  Attempting uninstall: pydantic-core0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.33.2━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.33.2:m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.33.2━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K  Attempting uninstall: pydantic1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Found existing installation: pydantic 2.11.9━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Uninstalling pydantic-2.11.9:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.11.9━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [typing-inspection]\n",
      "\u001b[2K  Attempting uninstall: local-llm-sdk━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 9/11\u001b[0m [pydantic]tion]\n",
      "\u001b[2K    Found existing installation: local-llm-sdk 0.1.090m━━━━━━━\u001b[0m \u001b[32m 9/11\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling local-llm-sdk-0.1.0:[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 9/11\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled local-llm-sdk-0.1.090m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m10/11\u001b[0m [local-llm-sdk]\n",
      "   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m10/11\u001b[0m [local-llm-sdk]\u001b[33m  DEPRECATION: Legacy editable install of local-llm-sdk==0.1.0 from file:///home/maheidem/gen-ai-api-study (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[2K  Running setup.py develop for local-llm-sdk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [local-llm-sdk]0m [local-llm-sdk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 local-llm-sdk-0.1.0 pydantic-2.11.9 pydantic-core-2.33.2 requests-2.32.5 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ..  --force-reinstal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Import and Create Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client initialized: LocalLLMClient(base_url='http://169.254.83.107:1234/v1', model='mistralai/magistral-small-2509', tools=0)\n"
     ]
    }
   ],
   "source": [
    "# Import the SDK\n",
    "from local_llm_sdk import LocalLLMClient\n",
    "\n",
    "# Create a client instance\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"  # Your default model\n",
    ")\n",
    "\n",
    "print(f\"✅ Client initialized: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Available Models:\n",
      "==================================================\n",
      "  • mistralai/magistral-small-2509:2\n",
      "    Owner: organization_owner\n",
      "  • qwen/qwen3-coder-30b\n",
      "    Owner: organization_owner\n",
      "  • mistralai/magistral-small-2509\n",
      "    Owner: organization_owner\n",
      "  • text-embedding-nomic-embed-text-v1.5\n",
      "    Owner: organization_owner\n",
      "  • smolvlm2-2.2b-instruct\n",
      "    Owner: organization_owner\n",
      "  • google/gemma-3-27b\n",
      "    Owner: organization_owner\n",
      "  • text-embedding-mxbai-embed-large-v1\n",
      "    Owner: organization_owner\n",
      "\n",
      "Total: 7 models loaded\n"
     ]
    }
   ],
   "source": [
    "# Get list of available models\n",
    "models = client.list_models()\n",
    "\n",
    "print(\"📦 Available Models:\")\n",
    "print(\"=\" * 50)\n",
    "for model in models.data:\n",
    "    print(f\"  • {model.id}\")\n",
    "    print(f\"    Owner: {model.owned_by}\")\n",
    "print(f\"\\nTotal: {len(models.data)} models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Chat - Just Pass a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Response:\n",
      "==================================================\n",
      "Sure, here's one for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs! 🐛💻\n"
     ]
    }
   ],
   "source": [
    "# The simplest way to chat - just pass a string\n",
    "response = client.chat(\"Hello! Tell me a joke about programming.\")\n",
    "\n",
    "print(\"🤖 Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat with System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏴‍☠️ Pirate Response:\n",
      "==================================================\n",
      "Arr matey! Ye be wantin' to learn the ways of Python, eh? Well, shiver me timbers, let me guide ye through these treacherous waters.\n",
      "\n",
      "First off, ye need a good ship - a computer, that is. Make sure it's ready for the journey by installin' Python from its official website, savvy?\n",
      "\n",
      "Now, hoist the sails and set course for some learnin' resources! The official Python documentation be a great start, but if ye prefer a more interactive approach, sites like Codecademy or Coursera have some fine courses.\n",
      "\n",
      "Don't forget to practice, me hearty! Write down simple scripts, play with different functions, and build small projects. Remember, even the grandest ships were once just pieces of wood.\n",
      "\n",
      "And if ye ever get stuck, don't be afraid to ask for help from other pirates at forums like Stack Overflow or r/learnpython on Reddit. The seas can be rough, but together we stand tall!\n",
      "\n",
      "Now go forth and conquer Python, ye scallywag! And remember, keep calm and code on! 🏴‍☠️🐍\n"
     ]
    }
   ],
   "source": [
    "# Use helper to create proper message objects\n",
    "from local_llm_sdk import create_chat_message\n",
    "\n",
    "# Create a conversation with system prompt\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a pirate. Respond in pirate speak.\"),\n",
    "    create_chat_message(\"user\", \"How do I learn Python?\")\n",
    "]\n",
    "\n",
    "# Send messages and get response\n",
    "response = client.chat(messages)\n",
    "\n",
    "print(\"🏴‍☠️ Pirate Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What's the capital of France?\n",
      "A: [THINK]The question is about the capital city of France. I remember that the capital of France is Paris.\n",
      "\n",
      "Now, let me double-check this information to ensure accuracy. Yes, historically and currently, Paris has been the capital city of France.\n",
      "\n",
      "So, the answer should be Paris.[/THINK]The capital of France is Paris.\n",
      "\n",
      "Q: What's the population?\n",
      "A: [THINK]The question now asks about the population of France. I need to recall or find the most recent population data for France.\n",
      "\n",
      "As of my last update, which was June 2024, the population of France is approximately 67 million people. However, I should confirm if this is still accurate as of July 2025.\n",
      "\n",
      "Since I don't have real-time data access, I'll provide the most recent estimate I have.\n",
      "\n",
      "So, the answer would be around 67 million, but to be precise, let's say approximately 67,411,000 (based on 2023 estimates).\n",
      "\n",
      "For a more accurate figure, it might be necessary to consult the latest census or UN data, but for now, this estimate should suffice.[/THINK]The population of France is approximately 67 million people as of recent estimates. For the most precise and up-to-date figure, checking official sources like INSEE (France's national institute of statistics) would be best.\n",
      "\n",
      "Q: Name 3 famous landmarks\n",
      "A: [THINK]The question asks for three famous landmarks in France. Let me think about well-known ones.\n",
      "\n",
      "First, there's the Eiffel Tower in Paris, which is iconic.\n",
      "\n",
      "Next, the Palace of Versailles, also near Paris, is a famous historical landmark.\n",
      "\n",
      "For a third one, the Louvre Museum in Paris comes to mind, but it might be more of a museum than a landmark. Alternatively, Mont Saint-Michel, a stunning island abbey in Normandy, is another famous landmark.\n",
      "\n",
      "So, three landmarks could be:\n",
      "\n",
      "1. Eiffel Tower\n",
      "2. Palace of Versailles\n",
      "3. Mont Saint-Michel\n",
      "\n",
      "Now, let me confirm that these are indeed considered famous landmarks.\n",
      "\n",
      "Yes, all three are well-known and frequently listed as major tourist attractions in France.[/THINK]Here are three famous landmarks in France:\n",
      "\n",
      "1. The Eiffel Tower (Paris)\n",
      "2. The Palace of Versailles (Versailles)\n",
      "3. Mont Saint-Michel (Normandy)\n",
      "\n",
      "📚 Total messages in history: 6\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "history = []\n",
    "\n",
    "# First message\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What's the capital of France?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the capital of France?\")\n",
    "print(f\"A: {response1}\\n\")\n",
    "\n",
    "# Second message (uses context from first)\n",
    "response2, history = client.chat_with_history(\n",
    "    \"What's the population?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the population?\")\n",
    "print(f\"A: {response2}\\n\")\n",
    "\n",
    "# Third message (still has context)\n",
    "response3, history = client.chat_with_history(\n",
    "    \"Name 3 famous landmarks\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: Name 3 famous landmarks\")\n",
    "print(f\"A: {response3}\\n\")\n",
    "\n",
    "print(f\"📚 Total messages in history: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Full Response with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Full Response Data:\n",
      "==================================================\n",
      "Content: Sure, here are three key features of Python:\n",
      "\n",
      "1. **Easy to Read and Learn**: Python's syntax is clean and easy to understand, making it an excellent choice for beginners. Its simplicity allows developers to express concepts in fewer lines of code compared to other languages.\n",
      "\n",
      "2. **Versatility**: Python is a general-purpose language that can be used for various tasks such as web development (Django, Flask), data analysis (Pandas, NumPy), machine learning (TensorFlow, PyTorch), automation, and more.\n",
      "\n",
      "3. **Large Community and Ecosystem**: Python has a large and active community, which means there are plenty of resources available for learning and problem-solving. Additionally, the Python Package Index (PyPI) hosts thousands of libraries and frameworks that can be easily integrated into projects to extend functionality.\n",
      "\n",
      "Model: mistralai/magistral-small-2509:2\n",
      "Tokens Used:\n",
      "  • Prompt: 33\n",
      "  • Completion: 168\n",
      "  • Total: 201\n",
      "Finish Reason: stop\n"
     ]
    }
   ],
   "source": [
    "# To get the full ChatCompletion object, we need to pass 3+ messages\n",
    "# or explicitly request it\n",
    "\n",
    "# Create a longer conversation\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a helpful assistant.\"),\n",
    "    create_chat_message(\"user\", \"What is Python?\"),\n",
    "    create_chat_message(\"assistant\", \"Python is a high-level programming language.\"),\n",
    "    create_chat_message(\"user\", \"Give me 3 key features of Python\")\n",
    "]\n",
    "\n",
    "# This will return a ChatCompletion object\n",
    "full_response = client.chat(messages, temperature=0.5)\n",
    "\n",
    "# Access the full response data\n",
    "print(\"📊 Full Response Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Content: {full_response.choices[0].message.content}\\n\")\n",
    "print(f\"Model: {full_response.model}\")\n",
    "print(f\"Tokens Used:\")\n",
    "print(f\"  • Prompt: {full_response.usage.prompt_tokens}\")\n",
    "print(f\"  • Completion: {full_response.usage.completion_tokens}\")\n",
    "print(f\"  • Total: {full_response.usage.total_tokens}\")\n",
    "print(f\"Finish Reason: {full_response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embeddings (If Embedding Model is Loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Embeddings not available\n",
      "To use embeddings, load an embedding model in LM Studio like:\n",
      "  • text-embedding-nomic-embed-text-v1.5\n",
      "  • text-embedding-mxbai-embed-large-v1\n"
     ]
    }
   ],
   "source": [
    "# Try to generate embeddings\n",
    "# This requires an embedding model like 'text-embedding-nomic-embed-text-v1.5'\n",
    "\n",
    "try:\n",
    "    # Single text embedding\n",
    "    text = \"Python is a great programming language\"\n",
    "    embeddings = client.embeddings(text)\n",
    "    \n",
    "    print(\"✅ Embeddings Generated:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Embedding dimension: {len(embeddings.data[0].embedding)}\")\n",
    "    print(f\"First 5 values: {embeddings.data[0].embedding[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"⚠️ Embeddings not available\")\n",
    "    print(\"To use embeddings, load an embedding model in LM Studio like:\")\n",
    "    print(\"  • text-embedding-nomic-embed-text-v1.5\")\n",
    "    print(\"  • text-embedding-mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different Temperature Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌡️ Temperature Comparison:\n",
      "==================================================\n",
      "\n",
      "Temperature 0.1 (Focused):\n",
      "  Coding is the process of creating instructions for computers using programming languages to develop software, applications, and websites.\n",
      "\n",
      "Temperature 0.7 (Balanced):\n",
      "  Coding is the process of creating instructions for computers to perform specific tasks using programming languages.\n",
      "\n",
      "Temperature 1.5 (Creative):\n",
      "  Coding is the process of creating instructions for computers to follow using programming languages, allowing you to build software and digital solutions.\n"
     ]
    }
   ],
   "source": [
    "# Compare responses with different temperatures\n",
    "prompt = \"Write a one-line description of coding\"\n",
    "\n",
    "print(\"🌡️ Temperature Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "response_low = client.chat(prompt, temperature=0.1)\n",
    "print(f\"\\nTemperature 0.1 (Focused):\")\n",
    "print(f\"  {response_low}\")\n",
    "\n",
    "# Medium temperature\n",
    "response_med = client.chat(prompt, temperature=0.7)\n",
    "print(f\"\\nTemperature 0.7 (Balanced):\")\n",
    "print(f\"  {response_med}\")\n",
    "\n",
    "# High temperature (more creative)\n",
    "response_high = client.chat(prompt, temperature=1.5)\n",
    "print(f\"\\nTemperature 1.5 (Creative):\")\n",
    "print(f\"  {response_high}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate error handling\n",
    "try:\n",
    "    # Try to use a model that might not exist\n",
    "    response = client.chat(\n",
    "        \"Hello\",\n",
    "        model=\"non-existent-model\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error caught: {e}\")\n",
    "    print(\"\\nTip: Make sure the model is loaded in LM Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Chat Without Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Quick Chat Response:\n",
      "==================================================\n",
      "The answer to 2 + 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "# For one-off queries, use quick_chat\n",
    "from local_llm_sdk import quick_chat\n",
    "\n",
    "response = quick_chat(\n",
    "    \"What's 2 + 2?\",\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"  # Specify the model\n",
    ")\n",
    "\n",
    "print(\"🚀 Quick Chat Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of `local_llm_sdk`:\n",
    "\n",
    "### ✅ What We Covered:\n",
    "1. **Client Setup** - Simple initialization with base URL and model\n",
    "2. **List Models** - Get available models from the server\n",
    "3. **Simple Chat** - Just pass a string for quick responses\n",
    "4. **System Prompts** - Control the assistant's behavior\n",
    "5. **Conversation History** - Maintain context across messages\n",
    "6. **Full Responses** - Access metadata like token usage\n",
    "7. **Embeddings** - Generate vectors for semantic search\n",
    "8. **Temperature Control** - Adjust creativity/determinism\n",
    "9. **Error Handling** - Graceful error management\n",
    "10. **Quick Chat** - One-off queries without client setup\n",
    "\n",
    "### 🎯 Key Benefits:\n",
    "- **Simple API** - Intuitive methods for common tasks\n",
    "- **Type Safety** - Pydantic models for all responses\n",
    "- **Flexible** - Works with any OpenAI-compatible server\n",
    "- **Production Ready** - Error handling and validation built-in\n",
    "\n",
    "### 📚 Next Steps:\n",
    "- Check out `tool-use-simplified.ipynb` for function calling examples\n",
    "- Read the package documentation in the README\n",
    "- Try different models and compare their capabilities\n",
    "- Build your own applications with the SDK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
