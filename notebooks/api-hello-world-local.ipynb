{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM SDK - Hello World\n",
    "\n",
    "This notebook demonstrates how to use the `local_llm_sdk` package to interact with LM Studio and other OpenAI-compatible local LLM servers.\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio running locally with API server enabled\n",
    "- Install the package: `pip install -e ..` (from notebooks directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Import and Create Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client initialized: LocalLLMClient(base_url='http://169.254.83.107:1234/v1', model='mistralai/magistral-small-2509', tools=0)\n"
     ]
    }
   ],
   "source": [
    "# Import the SDK\n",
    "from local_llm_sdk import LocalLLMClient\n",
    "\n",
    "# Create a client instance\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"  # Your default model\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Client initialized: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Available Models:\n",
      "==================================================\n",
      "  ‚Ä¢ mistralai/magistral-small-2509:2\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ qwen/qwen3-coder-30b\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ mistralai/magistral-small-2509\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ text-embedding-nomic-embed-text-v1.5\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ smolvlm2-2.2b-instruct\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ google/gemma-3-27b\n",
      "    Owner: organization_owner\n",
      "  ‚Ä¢ text-embedding-mxbai-embed-large-v1\n",
      "    Owner: organization_owner\n",
      "\n",
      "Total: 7 models loaded\n"
     ]
    }
   ],
   "source": [
    "# Get list of available models\n",
    "models = client.list_models()\n",
    "\n",
    "print(\"üì¶ Available Models:\")\n",
    "print(\"=\" * 50)\n",
    "for model in models.data:\n",
    "    print(f\"  ‚Ä¢ {model.id}\")\n",
    "    print(f\"    Owner: {model.owned_by}\")\n",
    "print(f\"\\nTotal: {len(models.data)} models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Chat - Just Pass a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Response:\n",
      "==================================================\n",
      "Sure, here's one for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs! üêõüíª\n"
     ]
    }
   ],
   "source": [
    "# The simplest way to chat - just pass a string\n",
    "response = client.chat(\"Hello! Tell me a joke about programming.\")\n",
    "\n",
    "print(\"ü§ñ Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat with System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¥‚Äç‚ò†Ô∏è Pirate Response:\n",
      "==================================================\n",
      "Arr matey! Ye be wantin' to learn the ways o' Python, eh? Well, here be some tips fer ye:\n",
      "\n",
      "1. **Start with the basics**: Get yerself familiar with variables, loops, and conditionals. Think of 'em as the ropes and sails o' yer ship.\n",
      "\n",
      "2. **Find a good guide**: There be many fine resources on the high seas of the internet. Websites like Codecademy, Coursera, or even the official Python docs can be yer treasure maps.\n",
      "\n",
      "3. **Practice, practice, practice!** Like swabbin' the deck, ye need to keep at it. Try writin' small programs, like calculators or simple games.\n",
      "\n",
      "4. **Join a crew**: Look fer online communities where ye can ask questions and learn from other pirates - er, coders. The Python subreddit and Stack Overflow be great places to start.\n",
      "\n",
      "5. **Build somethin' real**: Once ye got some skills, try buildin' somethin' that matters to ye. It could be a tool fer yer ship or a game fer yer crew.\n",
      "\n",
      "6. **Keep learnin'**: Python be a vast ocean, and there be always more to explore. Keep pushin' yerself to learn new things.\n",
      "\n",
      "Now go on, me hearty! Set sail on the high seas o' code and ye'll be a Python pro in no time!\n"
     ]
    }
   ],
   "source": [
    "# Use helper to create proper message objects\n",
    "from local_llm_sdk import create_chat_message\n",
    "\n",
    "# Create a conversation with system prompt\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a pirate. Respond in pirate speak.\"),\n",
    "    create_chat_message(\"user\", \"How do I learn Python?\")\n",
    "]\n",
    "\n",
    "# Send messages and get response\n",
    "response = client.chat(messages)\n",
    "\n",
    "print(\"üè¥‚Äç‚ò†Ô∏è Pirate Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What's the capital of France?\n",
      "A: [THINK]Alright, I remember that France is a country in Europe. What's its capital? Is it Paris? Yes, I think Paris is the capital of France.\n",
      "\n",
      "Now, to confirm this, let me double-check some facts. The Eiffel Tower is in Paris, and it's often associated with being the capital. Also, historically, Paris has been the political center of France.\n",
      "\n",
      "Okay, so my response should be that the capital of France is Paris.[/THINK]The capital of France is Paris.\n",
      "\n",
      "Q: What's the population?\n",
      "A: [THINK]Now, what about the population of Paris? I remember that it's a large city, but I'm not sure of the exact number. Maybe around 2 million? But that seems low for such a big city.\n",
      "\n",
      "I recall that the population of Paris itself (within the city limits) is different from the metropolitan area. The city proper might be about 2 million, while the metro area is much larger, maybe around 10-12 million?\n",
      "\n",
      "But to be precise, I need to recall or estimate the most recent figure. As of my last update in October 2023, the population of Paris was approximately 2.1 million within city limits and about 11 million in the metropolitan area.\n",
      "\n",
      "For this question, if it's asking about the city proper, I'll provide that number.[/THINK]The population of Paris is approximately 2.1 million people (within city limits). The metropolitan area has a much larger population, around 11-12 million.\n",
      "\n",
      "Q: Name 3 famous landmarks\n",
      "A: [THINK]Okay, France has many famous landmarks. Let me think of three.\n",
      "\n",
      "First, the Eiffel Tower in Paris is probably the most famous. It's an iconic iron tower that was built for the 1889 World's Fair.\n",
      "\n",
      "Second, the Louvre Museum in Paris is also very famous. It houses thousands of works of art, including the Mona Lisa.\n",
      "\n",
      "Third, how about the Palace of Versailles? It's a grand royal ch√¢teau that was the seat of political power in France under King Louis XIV.\n",
      "\n",
      "So, three famous landmarks could be:\n",
      "1. Eiffel Tower\n",
      "2. Louvre Museum\n",
      "3. Palace of Versailles\n",
      "\n",
      "But just to make sure, are there others that might be more famous? Maybe Notre-Dame Cathedral is also very well-known, as it's an important religious and cultural landmark in Paris.\n",
      "\n",
      "Let me stick with the first three I thought of for simplicity.\n",
      "\n",
      "Now, to present them clearly:\n",
      "\n",
      "1. Eiffel Tower\n",
      "2. Louvre Museum\n",
      "3. Palace of Versailles[/THINK]Here are three famous landmarks in France:\n",
      "\n",
      "1. The Eiffel Tower (Paris)\n",
      "2. The Louvre Museum (Paris)\n",
      "3. The Palace of Versailles\n",
      "\n",
      "üìö Total messages in history: 6\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "history = []\n",
    "\n",
    "# First message\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What's the capital of France?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the capital of France?\")\n",
    "print(f\"A: {response1}\\n\")\n",
    "\n",
    "# Second message (uses context from first)\n",
    "response2, history = client.chat_with_history(\n",
    "    \"What's the population?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the population?\")\n",
    "print(f\"A: {response2}\\n\")\n",
    "\n",
    "# Third message (still has context)\n",
    "response3, history = client.chat_with_history(\n",
    "    \"Name 3 famous landmarks\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: Name 3 famous landmarks\")\n",
    "print(f\"A: {response3}\\n\")\n",
    "\n",
    "print(f\"üìö Total messages in history: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Full Response with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Full Response Data:\n",
      "==================================================\n",
      "Content: Sure! Here are three key features of Python:\n",
      "\n",
      "1. **Easy to Read and Write**: Python's syntax is clear and concise, making it easy for beginners to learn and for experienced programmers to maintain.\n",
      "\n",
      "2. **Versatile and Extensible**: Python supports multiple programming paradigms (procedural, object-oriented, functional) and has a vast standard library that can be extended with additional modules.\n",
      "\n",
      "3. **Cross-Platform Compatibility**: Python is available on various platforms like Windows, macOS, Linux, etc., making it suitable for developing applications that need to run across different operating systems.\n",
      "\n",
      "Model: mistralai/magistral-small-2509:2\n",
      "Tokens Used:\n",
      "  ‚Ä¢ Prompt: 33\n",
      "  ‚Ä¢ Completion: 122\n",
      "  ‚Ä¢ Total: 155\n",
      "Finish Reason: stop\n"
     ]
    }
   ],
   "source": [
    "# To get the full ChatCompletion object, we need to pass 3+ messages\n",
    "# or explicitly request it\n",
    "\n",
    "# Create a longer conversation\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a helpful assistant.\"),\n",
    "    create_chat_message(\"user\", \"What is Python?\"),\n",
    "    create_chat_message(\"assistant\", \"Python is a high-level programming language.\"),\n",
    "    create_chat_message(\"user\", \"Give me 3 key features of Python\")\n",
    "]\n",
    "\n",
    "# This will return a ChatCompletion object\n",
    "full_response = client.chat(messages, temperature=0.5)\n",
    "\n",
    "# Access the full response data\n",
    "print(\"üìä Full Response Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Content: {full_response.choices[0].message.content}\\n\")\n",
    "print(f\"Model: {full_response.model}\")\n",
    "print(f\"Tokens Used:\")\n",
    "print(f\"  ‚Ä¢ Prompt: {full_response.usage.prompt_tokens}\")\n",
    "print(f\"  ‚Ä¢ Completion: {full_response.usage.completion_tokens}\")\n",
    "print(f\"  ‚Ä¢ Total: {full_response.usage.total_tokens}\")\n",
    "print(f\"Finish Reason: {full_response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embeddings (If Embedding Model is Loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Embeddings not available\n",
      "To use embeddings, load an embedding model in LM Studio like:\n",
      "  ‚Ä¢ text-embedding-nomic-embed-text-v1.5\n",
      "  ‚Ä¢ text-embedding-mxbai-embed-large-v1\n"
     ]
    }
   ],
   "source": [
    "# Try to generate embeddings\n",
    "# This requires an embedding model like 'text-embedding-nomic-embed-text-v1.5'\n",
    "\n",
    "try:\n",
    "    # Single text embedding\n",
    "    text = \"Python is a great programming language\"\n",
    "    embeddings = client.embeddings(text)\n",
    "    \n",
    "    print(\"‚úÖ Embeddings Generated:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Embedding dimension: {len(embeddings.data[0].embedding)}\")\n",
    "    print(f\"First 5 values: {embeddings.data[0].embedding[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available\")\n",
    "    print(\"To use embeddings, load an embedding model in LM Studio like:\")\n",
    "    print(\"  ‚Ä¢ text-embedding-nomic-embed-text-v1.5\")\n",
    "    print(\"  ‚Ä¢ text-embedding-mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different Temperature Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå°Ô∏è Temperature Comparison:\n",
      "==================================================\n",
      "\n",
      "Temperature 0.1 (Focused):\n",
      "  Coding is the process of creating instructions for computers using programming languages to develop software, websites, and applications.\n",
      "\n",
      "Temperature 0.7 (Balanced):\n",
      "  \"Coding is the process of writing instructions for computers using programming languages.\"\n",
      "\n",
      "Temperature 1.5 (Creative):\n",
      "  Coding is the process of creating instructions for a computer using programming languages to perform specific tasks.\n"
     ]
    }
   ],
   "source": [
    "# Compare responses with different temperatures\n",
    "prompt = \"Write a one-line description of coding\"\n",
    "\n",
    "print(\"üå°Ô∏è Temperature Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "response_low = client.chat(prompt, temperature=0.1)\n",
    "print(f\"\\nTemperature 0.1 (Focused):\")\n",
    "print(f\"  {response_low}\")\n",
    "\n",
    "# Medium temperature\n",
    "response_med = client.chat(prompt, temperature=0.7)\n",
    "print(f\"\\nTemperature 0.7 (Balanced):\")\n",
    "print(f\"  {response_med}\")\n",
    "\n",
    "# High temperature (more creative)\n",
    "response_high = client.chat(prompt, temperature=1.5)\n",
    "print(f\"\\nTemperature 1.5 (Creative):\")\n",
    "print(f\"  {response_high}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate error handling\n",
    "try:\n",
    "    # Try to use a model that might not exist\n",
    "    response = client.chat(\n",
    "        \"Hello\",\n",
    "        model=\"non-existent-model\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {e}\")\n",
    "    print(\"\\nTip: Make sure the model is loaded in LM Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Chat Without Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# For one-off queries, use quick_chat\nfrom local_llm_sdk import quick_chat\n\nresponse = quick_chat(\n    \"What's 2 + 2?\",\n    base_url=\"http://169.254.83.107:1234/v1\",\n    model=\"mistralai/magistral-small-2509\"  # Specify the model\n)\n\nprint(\"üöÄ Quick Chat Response:\")\nprint(\"=\" * 50)\nprint(response)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of `local_llm_sdk`:\n",
    "\n",
    "### ‚úÖ What We Covered:\n",
    "1. **Client Setup** - Simple initialization with base URL and model\n",
    "2. **List Models** - Get available models from the server\n",
    "3. **Simple Chat** - Just pass a string for quick responses\n",
    "4. **System Prompts** - Control the assistant's behavior\n",
    "5. **Conversation History** - Maintain context across messages\n",
    "6. **Full Responses** - Access metadata like token usage\n",
    "7. **Embeddings** - Generate vectors for semantic search\n",
    "8. **Temperature Control** - Adjust creativity/determinism\n",
    "9. **Error Handling** - Graceful error management\n",
    "10. **Quick Chat** - One-off queries without client setup\n",
    "\n",
    "### üéØ Key Benefits:\n",
    "- **Simple API** - Intuitive methods for common tasks\n",
    "- **Type Safety** - Pydantic models for all responses\n",
    "- **Flexible** - Works with any OpenAI-compatible server\n",
    "- **Production Ready** - Error handling and validation built-in\n",
    "\n",
    "### üìö Next Steps:\n",
    "- Check out `tool-use-simplified.ipynb` for function calling examples\n",
    "- Read the package documentation in the README\n",
    "- Try different models and compare their capabilities\n",
    "- Build your own applications with the SDK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}