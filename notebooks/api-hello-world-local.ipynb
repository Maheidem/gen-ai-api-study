{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34363cc4",
   "metadata": {},
   "source": [
    "# Local LLM SDK - Hello World\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the simplicity and power of the `local_llm_sdk` package for interacting with LM Studio and other OpenAI-compatible local LLM servers.\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio (or compatible server) running locally with API enabled\n",
    "- The `local_llm_sdk` package installed (`pip install -e ..` from notebooks directory)\n",
    "\n",
    "## What You'll Learn\n",
    "- How to create a client with one line of code\n",
    "- Simple chat interactions\n",
    "- Advanced features with type safety\n",
    "- Conversation history management\n",
    "- Embeddings generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gjpy5lzp5yr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the package is not installed, run this cell first (uncomment the line below)\n",
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e1ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LocalLLMClient from our SDK\n",
    "from local_llm_sdk import LocalLLMClient, create_client\n",
    "\n",
    "# That's it! We're ready to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjwyuwch46",
   "metadata": {},
   "source": [
    "## 2. Create the Client\n",
    "\n",
    "One line to create a fully-featured, type-safe client!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18bb306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client ready: LocalLLMClient(base_url='http://169.254.83.107:1234/v1', model='mistralai/magistral-small-2509', tools=0)\n"
     ]
    }
   ],
   "source": [
    "# Create a client with your LM Studio URL\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"  # Default model to use\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Client ready: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eeg2pown",
   "metadata": {},
   "source": [
    "## 3. List Available Models\n",
    "\n",
    "The client handles all the HTTP details for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8a4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Available models in local server:\n",
      "----------------------------------------\n",
      "‚Ä¢ mistralai/magistral-small-2509 (by organization_owner)\n",
      "‚Ä¢ qwen/qwen3-coder-30b (by organization_owner)\n",
      "‚Ä¢ text-embedding-nomic-embed-text-v1.5 (by organization_owner)\n",
      "‚Ä¢ smolvlm2-2.2b-instruct (by organization_owner)\n",
      "‚Ä¢ google/gemma-3-27b (by organization_owner)\n",
      "‚Ä¢ text-embedding-mxbai-embed-large-v1 (by organization_owner)\n",
      "\n",
      "‚ú® Total models: 6\n"
     ]
    }
   ],
   "source": [
    "# List models with one method call - returns typed ModelList object\n",
    "models = client.list_models()\n",
    "\n",
    "print(\"üì¶ Available models in local server:\")\n",
    "print(\"-\" * 40)\n",
    "for model in models.data:\n",
    "    print(f\"‚Ä¢ {model.id} (by {model.owned_by})\")\n",
    "    \n",
    "print(f\"\\n‚ú® Total models: {len(models.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61aa2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model Response:\n",
      "----------------------------------------\n",
      "Hello, World! It seems like we're in the middle of a code that's about to output some serious laughter! üòÑ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# Simple chat - just pass a string!\n",
    "response = client.chat(\"Reply to my hello world in a funny way!\")\n",
    "\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(\"-\" * 40)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zvj0d1xc8h",
   "metadata": {},
   "source": [
    "## 4. Simple Chat - The Easy Way\n",
    "\n",
    "Just pass a string and get a response. The client handles everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809dc170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Haiku:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù Haiku:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfull_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Metadata:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚Ä¢ Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_response.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "# Import helper for creating messages\n",
    "from local_llm_sdk import create_chat_message\n",
    "\n",
    "# Create a conversation with proper messages\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a helpful and creative assistant.\"),\n",
    "    create_chat_message(\"user\", \"Write a haiku about Python programming.\")\n",
    "]\n",
    "\n",
    "# Get full ChatCompletion object with all metadata\n",
    "full_response = client.chat(messages, temperature=0.7)\n",
    "\n",
    "# Access typed response data\n",
    "print(\"üìù Haiku:\")\n",
    "print(\"-\" * 40)\n",
    "print(full_response.choices[0].message.content)\n",
    "print(\"\\nüìä Metadata:\")\n",
    "print(f\"‚Ä¢ Model: {full_response.model}\")\n",
    "print(f\"‚Ä¢ Tokens used: {full_response.usage.total_tokens}\")\n",
    "print(f\"‚Ä¢ Finish reason: {full_response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taslcm7ql3",
   "metadata": {},
   "source": [
    "## 6. Conversation with History\n",
    "\n",
    "The client can manage conversation context for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g8nfelh1iq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "history = []\n",
    "\n",
    "# First message\n",
    "response1, history = client.chat_with_history(\"What's the capital of France?\", history)\n",
    "print(\"Q1: What's the capital of France?\")\n",
    "print(f\"A1: {response1}\\n\")\n",
    "\n",
    "# Follow-up that uses context\n",
    "response2, history = client.chat_with_history(\"What's its population?\", history)\n",
    "print(\"Q2: What's its population?\")\n",
    "print(f\"A2: {response2}\\n\")\n",
    "\n",
    "# Another follow-up\n",
    "response3, history = client.chat_with_history(\"Name 3 famous landmarks there\", history)\n",
    "print(\"Q3: Name 3 famous landmarks there\")\n",
    "print(f\"A3: {response3}\\n\")\n",
    "\n",
    "print(f\"üìö Conversation history now has {len(history)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gd7bb5x1e1g",
   "metadata": {},
   "source": [
    "## 7. Generate Embeddings\n",
    "\n",
    "Create vector embeddings for semantic search and similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2klng28mm9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for text (requires an embedding model to be loaded)\n",
    "try:\n",
    "    # Single text\n",
    "    embeddings = client.embeddings(\"Hello, world!\")\n",
    "    \n",
    "    print(f\"üìê Embedding generated:\")\n",
    "    print(f\"‚Ä¢ Dimension: {len(embeddings.data[0].embedding)}\")\n",
    "    print(f\"‚Ä¢ Model used: {embeddings.model}\")\n",
    "    print(f\"‚Ä¢ First 5 values: {embeddings.data[0].embedding[:5]}\")\n",
    "    \n",
    "    # Multiple texts at once\n",
    "    texts = [\"Python is great\", \"I love programming\", \"Hello world\"]\n",
    "    multi_embeddings = client.embeddings(texts)\n",
    "    print(f\"\\nüìê Generated {len(multi_embeddings.data)} embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Embeddings require an embedding model to be loaded\")\n",
    "    print(f\"   Load a model like 'text-embedding-nomic-embed-text-v1.5' in LM Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhgy4snvb6",
   "metadata": {},
   "source": [
    "## 8. Quick Chat Without Client\n",
    "\n",
    "For one-off queries without creating a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cl2gtsetvrd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the quick_chat function for one-off queries\n",
    "from local_llm_sdk import quick_chat\n",
    "\n",
    "response = quick_chat(\n",
    "    \"What's 2 + 2?\", \n",
    "    base_url=\"http://169.254.83.107:1234/v1\"\n",
    ")\n",
    "\n",
    "print(f\"Quick answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4tquqspi43u",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "Using the `local_llm_sdk` package, we showed how easy it is to:\n",
    "\n",
    "1. **Create a client** - One line of code\n",
    "2. **List models** - Type-safe response objects\n",
    "3. **Simple chat** - Just pass a string\n",
    "4. **Advanced chat** - Full control with ChatCompletion objects\n",
    "5. **Conversation history** - Automatic context management\n",
    "6. **Embeddings** - Vector generation for semantic search\n",
    "7. **Quick chat** - One-off queries without a client\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "‚úÖ **Type Safety** - All responses are validated Pydantic models\n",
    "‚úÖ **Simple Interface** - Complex operations in one method call\n",
    "‚úÖ **Flexible** - Simple mode for quick tasks, advanced mode for control\n",
    "‚úÖ **Production Ready** - Error handling, retries, and timeouts built-in\n",
    "\n",
    "### Compare: Before vs After\n",
    "\n",
    "**Before (Raw Requests):**\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\n",
    "    \"http://localhost:1234/v1/chat/completions\",\n",
    "    json={\n",
    "        \"model\": \"model-name\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    }\n",
    ")\n",
    "data = response.json()\n",
    "print(data[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "**After (LocalLLMClient):**\n",
    "```python\n",
    "from local_llm_sdk import LocalLLMClient\n",
    "client = LocalLLMClient()\n",
    "print(client.chat(\"Hello\"))\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the `tool-use-simplified.ipynb` notebook to see tool/function calling\n",
    "- Check the package documentation for more advanced features\n",
    "- Try different models and compare their capabilities\n",
    "- Build your own applications with the SDK!\n",
    "\n",
    "### üöÄ Ready for Production\n",
    "\n",
    "The `local_llm_sdk` is designed to be a production-ready SDK for local LLM interactions, providing the same developer experience as cloud APIs but with the privacy and control of local deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leb4nsp4txi",
   "metadata": {},
   "source": [
    "## 5. Advanced Chat with Full Response Object\n",
    "\n",
    "When you need more control and information, use messages and get the full ChatCompletion object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
