{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM SDK - Hello World\n",
    "\n",
    "This notebook demonstrates how to use the `local_llm_sdk` package to interact with LM Studio and other OpenAI-compatible local LLM servers.\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio running locally with API server enabled\n",
    "- Install the package: `pip install -e ..` (from notebooks directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Import and Create Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Client initialized: LocalLLMClient(base_url='http://169.254.83.107:1234/v1', model='mistralai/magistral-small-2509', tools=0)\n"
     ]
    }
   ],
   "source": [
    "# Import the SDK\n",
    "from local_llm_sdk import LocalLLMClient\n",
    "\n",
    "# Create a client instance\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"mistralai/magistral-small-2509\"  # Your default model\n",
    ")\n",
    "\n",
    "print(f\"âœ… Client initialized: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Available Models:\n",
      "==================================================\n",
      "  â€¢ mistralai/magistral-small-2509:2\n",
      "    Owner: organization_owner\n",
      "  â€¢ qwen/qwen3-coder-30b\n",
      "    Owner: organization_owner\n",
      "  â€¢ mistralai/magistral-small-2509\n",
      "    Owner: organization_owner\n",
      "  â€¢ text-embedding-nomic-embed-text-v1.5\n",
      "    Owner: organization_owner\n",
      "  â€¢ smolvlm2-2.2b-instruct\n",
      "    Owner: organization_owner\n",
      "  â€¢ google/gemma-3-27b\n",
      "    Owner: organization_owner\n",
      "  â€¢ text-embedding-mxbai-embed-large-v1\n",
      "    Owner: organization_owner\n",
      "\n",
      "Total: 7 models loaded\n"
     ]
    }
   ],
   "source": [
    "# Get list of available models\n",
    "models = client.list_models()\n",
    "\n",
    "print(\"ðŸ“¦ Available Models:\")\n",
    "print(\"=\" * 50)\n",
    "for model in models.data:\n",
    "    print(f\"  â€¢ {model.id}\")\n",
    "    print(f\"    Owner: {model.owned_by}\")\n",
    "print(f\"\\nTotal: {len(models.data)} models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Chat - Just Pass a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response:\n",
      "==================================================\n",
      "Sure, here's one for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs! ðŸ›ðŸ’»\n"
     ]
    }
   ],
   "source": [
    "# The simplest way to chat - just pass a string\n",
    "response = client.chat(\"Hello! Tell me a joke about programming.\")\n",
    "\n",
    "print(\"ðŸ¤– Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat with System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ´â€â˜ ï¸ Pirate Response:\n",
      "==================================================\n",
      "Arr matey! Ye be wantin' to learn the ways o' Python, eh? Well, here be some tips fer ye:\n",
      "\n",
      "1. **Start with the basics**: Get yerself familiar with variables, loops, and conditionals. Think of 'em as the ropes and sails o' yer ship.\n",
      "\n",
      "2. **Find a good guide**: There be many fine resources on the high seas of the internet. Websites like Codecademy, Coursera, or even the official Python docs can be yer treasure maps.\n",
      "\n",
      "3. **Practice, practice, practice!** Like swabbin' the deck, ye need to keep at it. Try writin' small programs, like calculators or simple games.\n",
      "\n",
      "4. **Join a crew**: Look fer online communities where ye can ask questions and learn from other pirates - er, coders. The Python subreddit and Stack Overflow be great places to start.\n",
      "\n",
      "5. **Build somethin' real**: Once ye got some skills, try buildin' somethin' that matters to ye. It could be a tool fer yer ship or a game fer yer crew.\n",
      "\n",
      "6. **Keep learnin'**: Python be a vast ocean, and there be always more to explore. Keep pushin' yerself to learn new things.\n",
      "\n",
      "Now go on, me hearty! Set sail on the high seas o' code and ye'll be a Python pro in no time!\n"
     ]
    }
   ],
   "source": [
    "# Use helper to create proper message objects\n",
    "from local_llm_sdk import create_chat_message\n",
    "\n",
    "# Create a conversation with system prompt\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a pirate. Respond in pirate speak.\"),\n",
    "    create_chat_message(\"user\", \"How do I learn Python?\")\n",
    "]\n",
    "\n",
    "# Send messages and get response\n",
    "response = client.chat(messages)\n",
    "\n",
    "print(\"ðŸ´â€â˜ ï¸ Pirate Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What's the capital of France?\n",
      "A: [THINK]Alright, I remember that France is a country in Europe. What's its capital? Is it Paris? Yes, I think Paris is the capital of France.\n",
      "\n",
      "Now, to confirm this, let me double-check some facts. The Eiffel Tower is in Paris, and it's often associated with being the capital. Also, historically, Paris has been the political center of France.\n",
      "\n",
      "Okay, so my response should be that the capital of France is Paris.[/THINK]The capital of France is Paris.\n",
      "\n",
      "Q: What's the population?\n",
      "A: [THINK]Now, what about the population of Paris? I remember that it's a large city, but I'm not sure of the exact number. Maybe around 2 million? But that seems low for such a big city.\n",
      "\n",
      "I recall that the population of Paris itself (within the city limits) is different from the metropolitan area. The city proper might be about 2 million, while the metro area is much larger, maybe around 10-12 million?\n",
      "\n",
      "But to be precise, I need to recall or estimate the most recent figure. As of my last update in October 2023, the population of Paris was approximately 2.1 million within city limits and about 11 million in the metropolitan area.\n",
      "\n",
      "For this question, if it's asking about the city proper, I'll provide that number.[/THINK]The population of Paris is approximately 2.1 million people (within city limits). The metropolitan area has a much larger population, around 11-12 million.\n",
      "\n",
      "Q: Name 3 famous landmarks\n",
      "A: [THINK]Okay, France has many famous landmarks. Let me think of three.\n",
      "\n",
      "First, the Eiffel Tower in Paris is probably the most famous. It's an iconic iron tower that was built for the 1889 World's Fair.\n",
      "\n",
      "Second, the Louvre Museum in Paris is also very famous. It houses thousands of works of art, including the Mona Lisa.\n",
      "\n",
      "Third, how about the Palace of Versailles? It's a grand royal chÃ¢teau that was the seat of political power in France under King Louis XIV.\n",
      "\n",
      "So, three famous landmarks could be:\n",
      "1. Eiffel Tower\n",
      "2. Louvre Museum\n",
      "3. Palace of Versailles\n",
      "\n",
      "But just to make sure, are there others that might be more famous? Maybe Notre-Dame Cathedral is also very well-known, as it's an important religious and cultural landmark in Paris.\n",
      "\n",
      "Let me stick with the first three I thought of for simplicity.\n",
      "\n",
      "Now, to present them clearly:\n",
      "\n",
      "1. Eiffel Tower\n",
      "2. Louvre Museum\n",
      "3. Palace of Versailles[/THINK]Here are three famous landmarks in France:\n",
      "\n",
      "1. The Eiffel Tower (Paris)\n",
      "2. The Louvre Museum (Paris)\n",
      "3. The Palace of Versailles\n",
      "\n",
      "ðŸ“š Total messages in history: 6\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "history = []\n",
    "\n",
    "# First message\n",
    "response1, history = client.chat_with_history(\n",
    "    \"What's the capital of France?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the capital of France?\")\n",
    "print(f\"A: {response1}\\n\")\n",
    "\n",
    "# Second message (uses context from first)\n",
    "response2, history = client.chat_with_history(\n",
    "    \"What's the population?\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: What's the population?\")\n",
    "print(f\"A: {response2}\\n\")\n",
    "\n",
    "# Third message (still has context)\n",
    "response3, history = client.chat_with_history(\n",
    "    \"Name 3 famous landmarks\", \n",
    "    history\n",
    ")\n",
    "print(\"Q: Name 3 famous landmarks\")\n",
    "print(f\"A: {response3}\\n\")\n",
    "\n",
    "print(f\"ðŸ“š Total messages in history: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Full Response with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Full Response Data:\n",
      "==================================================\n",
      "Content: Sure! Here are three key features of Python:\n",
      "\n",
      "1. **Easy to Read and Write**: Python's syntax is clear and concise, making it easy for beginners to learn and for experienced programmers to maintain.\n",
      "\n",
      "2. **Versatile and Extensible**: Python supports multiple programming paradigms (procedural, object-oriented, functional) and has a vast standard library that can be extended with additional modules.\n",
      "\n",
      "3. **Cross-Platform Compatibility**: Python is available on various platforms like Windows, macOS, Linux, etc., making it suitable for developing applications that need to run across different operating systems.\n",
      "\n",
      "Model: mistralai/magistral-small-2509:2\n",
      "Tokens Used:\n",
      "  â€¢ Prompt: 33\n",
      "  â€¢ Completion: 122\n",
      "  â€¢ Total: 155\n",
      "Finish Reason: stop\n"
     ]
    }
   ],
   "source": [
    "# To get the full ChatCompletion object, we need to pass 3+ messages\n",
    "# or explicitly request it\n",
    "\n",
    "# Create a longer conversation\n",
    "messages = [\n",
    "    create_chat_message(\"system\", \"You are a helpful assistant.\"),\n",
    "    create_chat_message(\"user\", \"What is Python?\"),\n",
    "    create_chat_message(\"assistant\", \"Python is a high-level programming language.\"),\n",
    "    create_chat_message(\"user\", \"Give me 3 key features of Python\")\n",
    "]\n",
    "\n",
    "# This will return a ChatCompletion object\n",
    "full_response = client.chat(messages, temperature=0.5)\n",
    "\n",
    "# Access the full response data\n",
    "print(\"ðŸ“Š Full Response Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Content: {full_response.choices[0].message.content}\\n\")\n",
    "print(f\"Model: {full_response.model}\")\n",
    "print(f\"Tokens Used:\")\n",
    "print(f\"  â€¢ Prompt: {full_response.usage.prompt_tokens}\")\n",
    "print(f\"  â€¢ Completion: {full_response.usage.completion_tokens}\")\n",
    "print(f\"  â€¢ Total: {full_response.usage.total_tokens}\")\n",
    "print(f\"Finish Reason: {full_response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embeddings (If Embedding Model is Loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Embeddings not available\n",
      "To use embeddings, load an embedding model in LM Studio like:\n",
      "  â€¢ text-embedding-nomic-embed-text-v1.5\n",
      "  â€¢ text-embedding-mxbai-embed-large-v1\n"
     ]
    }
   ],
   "source": [
    "# Try to generate embeddings\n",
    "# This requires an embedding model like 'text-embedding-nomic-embed-text-v1.5'\n",
    "\n",
    "try:\n",
    "    # Single text embedding\n",
    "    text = \"Python is a great programming language\"\n",
    "    embeddings = client.embeddings(text)\n",
    "    \n",
    "    print(\"âœ… Embeddings Generated:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Embedding dimension: {len(embeddings.data[0].embedding)}\")\n",
    "    print(f\"First 5 values: {embeddings.data[0].embedding[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Embeddings not available\")\n",
    "    print(\"To use embeddings, load an embedding model in LM Studio like:\")\n",
    "    print(\"  â€¢ text-embedding-nomic-embed-text-v1.5\")\n",
    "    print(\"  â€¢ text-embedding-mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different Temperature Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ¡ï¸ Temperature Comparison:\n",
      "==================================================\n",
      "\n",
      "Temperature 0.1 (Focused):\n",
      "  Coding is the process of creating instructions for computers using programming languages to develop software, websites, and applications.\n",
      "\n",
      "Temperature 0.7 (Balanced):\n",
      "  \"Coding is the process of writing instructions for computers using programming languages.\"\n",
      "\n",
      "Temperature 1.5 (Creative):\n",
      "  Coding is the process of creating instructions for a computer using programming languages to perform specific tasks.\n"
     ]
    }
   ],
   "source": [
    "# Compare responses with different temperatures\n",
    "prompt = \"Write a one-line description of coding\"\n",
    "\n",
    "print(\"ðŸŒ¡ï¸ Temperature Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "response_low = client.chat(prompt, temperature=0.1)\n",
    "print(f\"\\nTemperature 0.1 (Focused):\")\n",
    "print(f\"  {response_low}\")\n",
    "\n",
    "# Medium temperature\n",
    "response_med = client.chat(prompt, temperature=0.7)\n",
    "print(f\"\\nTemperature 0.7 (Balanced):\")\n",
    "print(f\"  {response_med}\")\n",
    "\n",
    "# High temperature (more creative)\n",
    "response_high = client.chat(prompt, temperature=1.5)\n",
    "print(f\"\\nTemperature 1.5 (Creative):\")\n",
    "print(f\"  {response_high}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate error handling\n",
    "try:\n",
    "    # Try to use a model that might not exist\n",
    "    response = client.chat(\n",
    "        \"Hello\",\n",
    "        model=\"non-existent-model\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error caught: {e}\")\n",
    "    print(\"\\nTip: Make sure the model is loaded in LM Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Chat Without Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatCompletionRequest\nmodel\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For one-off queries, use quick_chat\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlocal_llm_sdk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quick_chat\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mquick_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms 2 + 2?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://169.254.83.107:1234/v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Quick Chat Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gen-ai-api-study/local_llm_sdk/client.py:272\u001b[39m, in \u001b[36mquick_chat\u001b[39m\u001b[34m(query, base_url)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Quick one-off chat without creating a client.\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m client = LocalLLMClient(base_url)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gen-ai-api-study/local_llm_sdk/client.py:224\u001b[39m, in \u001b[36mLocalLLMClient.chat_simple\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat_simple\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03m    Simple chat without tool support or conversation history.\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m \u001b[33;03m        Model's response as string\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gen-ai-api-study/local_llm_sdk/client.py:114\u001b[39m, in \u001b[36mLocalLLMClient.chat\u001b[39m\u001b[34m(self, messages, model, temperature, max_tokens, use_tools, stream, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     messages = [\n\u001b[32m    109\u001b[39m         create_chat_message(\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant with access to tools.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    110\u001b[39m         create_chat_message(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, messages)\n\u001b[32m    111\u001b[39m     ]\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Build request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m request = \u001b[43mcreate_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Add tools if available and requested\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_tools \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tools.list_tools():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gen-ai-api-study/local_llm_sdk/models.py:289\u001b[39m, in \u001b[36mcreate_chat_completion_request\u001b[39m\u001b[34m(model, messages, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_chat_completion_request\u001b[39m(\n\u001b[32m    274\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    275\u001b[39m     messages: List[ChatMessage],\n\u001b[32m    276\u001b[39m     **kwargs\n\u001b[32m    277\u001b[39m ) -> ChatCompletionRequest:\n\u001b[32m    278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    Helper function to create a chat completion request.\u001b[39;00m\n\u001b[32m    280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    287\u001b[39m \u001b[33;03m        ChatCompletionRequest instance\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatCompletionRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gen-ai/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatCompletionRequest\nmodel\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "# For one-off queries, use quick_chat\n",
    "from local_llm_sdk import quick_chat\n",
    "\n",
    "response = quick_chat(\n",
    "    \"What's 2 + 2?\",\n",
    "    base_url=\"http://169.254.83.107:1234/v1\"\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Quick Chat Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of `local_llm_sdk`:\n",
    "\n",
    "### âœ… What We Covered:\n",
    "1. **Client Setup** - Simple initialization with base URL and model\n",
    "2. **List Models** - Get available models from the server\n",
    "3. **Simple Chat** - Just pass a string for quick responses\n",
    "4. **System Prompts** - Control the assistant's behavior\n",
    "5. **Conversation History** - Maintain context across messages\n",
    "6. **Full Responses** - Access metadata like token usage\n",
    "7. **Embeddings** - Generate vectors for semantic search\n",
    "8. **Temperature Control** - Adjust creativity/determinism\n",
    "9. **Error Handling** - Graceful error management\n",
    "10. **Quick Chat** - One-off queries without client setup\n",
    "\n",
    "### ðŸŽ¯ Key Benefits:\n",
    "- **Simple API** - Intuitive methods for common tasks\n",
    "- **Type Safety** - Pydantic models for all responses\n",
    "- **Flexible** - Works with any OpenAI-compatible server\n",
    "- **Production Ready** - Error handling and validation built-in\n",
    "\n",
    "### ðŸ“š Next Steps:\n",
    "- Check out `tool-use-simplified.ipynb` for function calling examples\n",
    "- Read the package documentation in the README\n",
    "- Try different models and compare their capabilities\n",
    "- Build your own applications with the SDK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
