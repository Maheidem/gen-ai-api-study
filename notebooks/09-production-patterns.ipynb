{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè≠ 09: Production Patterns\n",
    "\n",
    "Learn production-ready patterns for building robust, reliable LLM applications with proper error handling, timeouts, and retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Implement comprehensive error handling with try/except\n",
    "- [ ] Configure timeouts to prevent hanging operations\n",
    "- [ ] Build retry logic with exponential backoff\n",
    "- [ ] Manage environment-specific configuration\n",
    "- [ ] Implement production-grade logging\n",
    "- [ ] Create robust API wrappers\n",
    "- [ ] Handle rate limiting and API errors gracefully\n",
    "- [ ] Build resilient agent workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prerequisites\n",
    "\n",
    "- Completed notebooks 02-07 (basic usage through agents)\n",
    "- Understanding of Python error handling\n",
    "- Familiarity with environment variables\n",
    "- Basic knowledge of logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Estimated Time: 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Why Production Patterns Matter\n",
    "\n",
    "**Development vs. Production:**\n",
    "\n",
    "| Development | Production |\n",
    "|-------------|------------|\n",
    "| ‚ùå Assume everything works | ‚úÖ Expect failures |\n",
    "| ‚ùå Let errors crash | ‚úÖ Handle errors gracefully |\n",
    "| ‚ùå Wait indefinitely | ‚úÖ Use timeouts |\n",
    "| ‚ùå Retry manually | ‚úÖ Automatic retry with backoff |\n",
    "| ‚ùå Print statements | ‚úÖ Structured logging |\n",
    "| ‚ùå Hardcoded values | ‚úÖ Environment configuration |\n",
    "\n",
    "**Common Production Issues:**\n",
    "- Network failures\n",
    "- API rate limits\n",
    "- Timeouts\n",
    "- Invalid responses\n",
    "- Model unavailability\n",
    "- Resource exhaustion\n",
    "\n",
    "**Goal:** Build systems that degrade gracefully and recover automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Error Handling Patterns\n",
    "\n",
    "Proper error handling is the foundation of production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm_sdk import LocalLLMClient\n",
    "from local_llm_sdk.exceptions import (\n",
    "    LLMError,\n",
    "    TimeoutError,\n",
    "    APIError,\n",
    "    ValidationError\n",
    ")\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create client\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    timeout=300  # 5 minutes\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Client configured with production settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_chat(prompt: str, client: LocalLLMClient) -> str:\n",
    "    \"\"\"\n",
    "    Chat with comprehensive error handling.\n",
    "    \n",
    "    Returns response on success, error message on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat(prompt)\n",
    "        logger.info(f\"Chat successful for prompt: {prompt[:50]}...\")\n",
    "        return response\n",
    "        \n",
    "    except TimeoutError as e:\n",
    "        logger.error(f\"Timeout occurred: {e}\")\n",
    "        return \"Error: Request timed out. Please try again with a simpler prompt.\"\n",
    "        \n",
    "    except APIError as e:\n",
    "        logger.error(f\"API error: {e}\")\n",
    "        return f\"Error: API request failed. Details: {str(e)}\"\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        return f\"Error: Invalid request. Details: {str(e)}\"\n",
    "        \n",
    "    except LLMError as e:\n",
    "        logger.error(f\"LLM error: {e}\")\n",
    "        return f\"Error: LLM service error. Details: {str(e)}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Unexpected error: {e}\")\n",
    "        return f\"Error: An unexpected error occurred. Please contact support.\"\n",
    "\n",
    "# Test it\n",
    "response = safe_chat(\"What is 5 + 5?\", client)\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ChatResult:\n",
    "    \"\"\"Structured result with success status and optional error.\"\"\"\n",
    "    success: bool\n",
    "    response: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    error_type: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "def robust_chat(prompt: str, client: LocalLLMClient) -> ChatResult:\n",
    "    \"\"\"\n",
    "    Chat with structured error handling.\n",
    "    \n",
    "    Returns ChatResult with success flag and details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat(prompt)\n",
    "        return ChatResult(\n",
    "            success=True,\n",
    "            response=response,\n",
    "            metadata={\"prompt_length\": len(prompt)}\n",
    "        )\n",
    "        \n",
    "    except TimeoutError as e:\n",
    "        return ChatResult(\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            error_type=\"timeout\"\n",
    "        )\n",
    "        \n",
    "    except APIError as e:\n",
    "        return ChatResult(\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            error_type=\"api_error\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ChatResult(\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            error_type=\"unknown\"\n",
    "        )\n",
    "\n",
    "# Test it\n",
    "result = robust_chat(\"Calculate 123 * 456\", client)\n",
    "\n",
    "if result.success:\n",
    "    print(f\"‚úÖ Success: {result.response}\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed: {result.error_type} - {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Timeout Configuration\n",
    "\n",
    "Timeouts prevent operations from hanging indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure different timeouts for different operations\n",
    "\n",
    "# Short timeout for simple queries\n",
    "quick_client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    timeout=30  # 30 seconds\n",
    ")\n",
    "\n",
    "# Standard timeout\n",
    "standard_client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    timeout=120  # 2 minutes\n",
    ")\n",
    "\n",
    "# Long timeout for complex operations\n",
    "patient_client = LocalLLMClient(\n",
    "    base_url=\"http://169.254.83.107:1234/v1\",\n",
    "    model=\"your-model-name\",\n",
    "    timeout=300  # 5 minutes\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Clients configured with different timeouts:\")\n",
    "print(\"   - Quick: 30s for simple queries\")\n",
    "print(\"   - Standard: 120s for typical operations\")\n",
    "print(\"   - Patient: 300s for complex tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation-Specific Timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_timeout(\n",
    "    prompt: str,\n",
    "    client: LocalLLMClient,\n",
    "    timeout: int = 60\n",
    ") -> ChatResult:\n",
    "    \"\"\"\n",
    "    Chat with configurable timeout.\n",
    "    \"\"\"\n",
    "    import signal\n",
    "    from contextlib import contextmanager\n",
    "    \n",
    "    @contextmanager\n",
    "    def time_limit(seconds):\n",
    "        def signal_handler(signum, frame):\n",
    "            raise TimeoutError(f\"Operation exceeded {seconds}s timeout\")\n",
    "        signal.signal(signal.SIGALRM, signal_handler)\n",
    "        signal.alarm(seconds)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            signal.alarm(0)\n",
    "    \n",
    "    try:\n",
    "        with time_limit(timeout):\n",
    "            response = client.chat(prompt)\n",
    "            return ChatResult(success=True, response=response)\n",
    "    except TimeoutError as e:\n",
    "        return ChatResult(success=False, error=str(e), error_type=\"timeout\")\n",
    "    except Exception as e:\n",
    "        return ChatResult(success=False, error=str(e), error_type=\"error\")\n",
    "\n",
    "# Use with different timeouts\n",
    "quick_result = chat_with_timeout(\"What is 2+2?\", client, timeout=10)\n",
    "print(f\"Quick query: {quick_result.success}\")\n",
    "\n",
    "complex_result = chat_with_timeout(\n",
    "    \"Explain quantum computing in detail\",\n",
    "    client,\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"Complex query: {complex_result.success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Retry Logic with Exponential Backoff\n",
    "\n",
    "Automatically retry failed operations with increasing delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "def exponential_backoff_retry(\n",
    "    func: Callable,\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    max_delay: float = 60.0,\n",
    "    retriable_exceptions: tuple = (APIError, TimeoutError)\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Retry function with exponential backoff.\n",
    "    \n",
    "    Delays: 1s, 2s, 4s, 8s, etc. (capped at max_delay)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "            \n",
    "        except retriable_exceptions as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                # Last attempt, re-raise\n",
    "                logger.error(f\"All {max_retries} retries exhausted\")\n",
    "                raise\n",
    "            \n",
    "            # Calculate delay with exponential backoff\n",
    "            delay = min(\n",
    "                initial_delay * (exponential_base ** attempt),\n",
    "                max_delay\n",
    "            )\n",
    "            \n",
    "            logger.warning(\n",
    "                f\"Attempt {attempt + 1}/{max_retries} failed: {e}. \"\n",
    "                f\"Retrying in {delay}s...\"\n",
    "            )\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Non-retriable error, fail immediately\n",
    "            logger.error(f\"Non-retriable error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "def make_chat_call():\n",
    "    return client.chat(\"What is artificial intelligence?\")\n",
    "\n",
    "try:\n",
    "    response = exponential_backoff_retry(make_chat_call, max_retries=3)\n",
    "    print(f\"‚úÖ Success: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed after retries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator Pattern for Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def retry_with_backoff(\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator for automatic retries with exponential backoff.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except (APIError, TimeoutError) as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "                    delay = initial_delay * (exponential_base ** attempt)\n",
    "                    logger.warning(f\"Retry {attempt + 1}/{max_retries} after {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Use as decorator\n",
    "@retry_with_backoff(max_retries=3, initial_delay=1.0)\n",
    "def resilient_chat(prompt: str) -> str:\n",
    "    \"\"\"Chat with automatic retries.\"\"\"\n",
    "    return client.chat(prompt)\n",
    "\n",
    "# Now this will automatically retry on failures\n",
    "response = resilient_chat(\"Explain machine learning briefly\")\n",
    "print(f\"Response: {response[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Environment Configuration\n",
    "\n",
    "Use environment variables for configuration, not hardcoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration management.\"\"\"\n",
    "    \n",
    "    # LLM Configuration\n",
    "    LLM_BASE_URL: str = os.getenv(\n",
    "        \"LLM_BASE_URL\",\n",
    "        \"http://169.254.83.107:1234/v1\"  # default\n",
    "    )\n",
    "    LLM_MODEL: str = os.getenv(\"LLM_MODEL\", \"default-model\")\n",
    "    LLM_TIMEOUT: int = int(os.getenv(\"LLM_TIMEOUT\", \"300\"))\n",
    "    LLM_TEMPERATURE: float = float(os.getenv(\"LLM_TEMPERATURE\", \"0.7\"))\n",
    "    \n",
    "    # Retry Configuration\n",
    "    MAX_RETRIES: int = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n",
    "    RETRY_INITIAL_DELAY: float = float(os.getenv(\"RETRY_INITIAL_DELAY\", \"1.0\"))\n",
    "    RETRY_MAX_DELAY: float = float(os.getenv(\"RETRY_MAX_DELAY\", \"60.0\"))\n",
    "    \n",
    "    # Feature Flags\n",
    "    ENABLE_TRACING: bool = os.getenv(\"ENABLE_TRACING\", \"false\").lower() == \"true\"\n",
    "    ENABLE_CACHING: bool = os.getenv(\"ENABLE_CACHING\", \"false\").lower() == \"true\"\n",
    "    \n",
    "    # Environment\n",
    "    ENV: str = os.getenv(\"ENV\", \"development\")  # development, staging, production\n",
    "    \n",
    "    @classmethod\n",
    "    def is_production(cls) -> bool:\n",
    "        return cls.ENV == \"production\"\n",
    "    \n",
    "    @classmethod\n",
    "    def is_development(cls) -> bool:\n",
    "        return cls.ENV == \"development\"\n",
    "\n",
    "# Use configuration\n",
    "config_client = LocalLLMClient(\n",
    "    base_url=Config.LLM_BASE_URL,\n",
    "    model=Config.LLM_MODEL,\n",
    "    timeout=Config.LLM_TIMEOUT,\n",
    "    temperature=Config.LLM_TEMPERATURE,\n",
    "    enable_tracing=Config.ENABLE_TRACING\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Client configured from environment:\")\n",
    "print(f\"   Base URL: {Config.LLM_BASE_URL}\")\n",
    "print(f\"   Timeout: {Config.LLM_TIMEOUT}s\")\n",
    "print(f\"   Max Retries: {Config.MAX_RETRIES}\")\n",
    "print(f\"   Environment: {Config.ENV}\")\n",
    "print(f\"   Tracing: {Config.ENABLE_TRACING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env File Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python-dotenv if needed:\n",
    "# !pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now environment variables are loaded from .env\n",
    "print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "\n",
    "# Example .env file:\n",
    "\"\"\"\n",
    "# .env\n",
    "LLM_BASE_URL=http://localhost:1234/v1\n",
    "LLM_MODEL=my-model\n",
    "LLM_TIMEOUT=300\n",
    "LLM_TEMPERATURE=0.7\n",
    "MAX_RETRIES=5\n",
    "ENABLE_TRACING=true\n",
    "ENV=production\n",
    "\"\"\"\n",
    "print(\"\\nüí° Create a .env file with the above configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Production Logging\n",
    "\n",
    "Structured logging for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductionLogger:\n",
    "    \"\"\"Structured logging for production.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure structured logging.\"\"\"\n",
    "        handler = logging.StreamHandler()\n",
    "        \n",
    "        # JSON formatter for structured logs\n",
    "        class JSONFormatter(logging.Formatter):\n",
    "            def format(self, record):\n",
    "                log_data = {\n",
    "                    'timestamp': datetime.utcnow().isoformat(),\n",
    "                    'level': record.levelname,\n",
    "                    'logger': record.name,\n",
    "                    'message': record.getMessage(),\n",
    "                }\n",
    "                \n",
    "                # Add exception info if present\n",
    "                if record.exc_info:\n",
    "                    log_data['exception'] = self.formatException(record.exc_info)\n",
    "                \n",
    "                # Add custom fields\n",
    "                if hasattr(record, 'extra_data'):\n",
    "                    log_data.update(record.extra_data)\n",
    "                \n",
    "                return json.dumps(log_data)\n",
    "        \n",
    "        handler.setFormatter(JSONFormatter())\n",
    "        self.logger.addHandler(handler)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "    \n",
    "    def log_llm_call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response: Optional[str],\n",
    "        duration_ms: float,\n",
    "        success: bool,\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Log LLM API call with metadata.\"\"\"\n",
    "        extra_data = {\n",
    "            'event_type': 'llm_call',\n",
    "            'prompt_length': len(prompt),\n",
    "            'response_length': len(response) if response else 0,\n",
    "            'duration_ms': duration_ms,\n",
    "            'success': success,\n",
    "        }\n",
    "        \n",
    "        if error:\n",
    "            extra_data['error'] = error\n",
    "        \n",
    "        log_record = self.logger.makeRecord(\n",
    "            self.logger.name,\n",
    "            logging.INFO if success else logging.ERROR,\n",
    "            \"\", 0, \"LLM call completed\", (), None\n",
    "        )\n",
    "        log_record.extra_data = extra_data\n",
    "        self.logger.handle(log_record)\n",
    "\n",
    "# Usage\n",
    "prod_logger = ProductionLogger(\"llm_app\")\n",
    "\n",
    "# Log a call\n",
    "start_time = time.time()\n",
    "response = client.chat(\"Hello!\")\n",
    "duration = (time.time() - start_time) * 1000\n",
    "\n",
    "prod_logger.log_llm_call(\n",
    "    prompt=\"Hello!\",\n",
    "    response=response,\n",
    "    duration_ms=duration,\n",
    "    success=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Call logged with structured metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Complete Production Wrapper\n",
    "\n",
    "Putting it all together: a production-ready API wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionLLMClient:\n",
    "    \"\"\"\n",
    "    Production-ready LLM client wrapper.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic retries with exponential backoff\n",
    "    - Comprehensive error handling\n",
    "    - Structured logging\n",
    "    - Timeout management\n",
    "    - Environment configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = LocalLLMClient(\n",
    "            base_url=Config.LLM_BASE_URL,\n",
    "            model=Config.LLM_MODEL,\n",
    "            timeout=Config.LLM_TIMEOUT,\n",
    "            temperature=Config.LLM_TEMPERATURE,\n",
    "            enable_tracing=Config.ENABLE_TRACING\n",
    "        )\n",
    "        self.logger = ProductionLogger(\"production_llm_client\")\n",
    "        self.max_retries = Config.MAX_RETRIES\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        timeout: Optional[int] = None,\n",
    "        max_retries: Optional[int] = None\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Chat with full production features.\n",
    "        \"\"\"\n",
    "        max_retries = max_retries or self.max_retries\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat(prompt)\n",
    "                duration_ms = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # Log success\n",
    "                self.logger.log_llm_call(\n",
    "                    prompt=prompt,\n",
    "                    response=response,\n",
    "                    duration_ms=duration_ms,\n",
    "                    success=True\n",
    "                )\n",
    "                \n",
    "                return ChatResult(\n",
    "                    success=True,\n",
    "                    response=response,\n",
    "                    metadata={\"duration_ms\": duration_ms, \"attempts\": attempt + 1}\n",
    "                )\n",
    "                \n",
    "            except (APIError, TimeoutError) as e:\n",
    "                duration_ms = (time.time() - start_time) * 1000\n",
    "                \n",
    "                if attempt == max_retries - 1:\n",
    "                    # Last attempt failed\n",
    "                    self.logger.log_llm_call(\n",
    "                        prompt=prompt,\n",
    "                        response=None,\n",
    "                        duration_ms=duration_ms,\n",
    "                        success=False,\n",
    "                        error=str(e)\n",
    "                    )\n",
    "                    \n",
    "                    return ChatResult(\n",
    "                        success=False,\n",
    "                        error=f\"Failed after {max_retries} attempts: {str(e)}\",\n",
    "                        error_type=type(e).__name__\n",
    "                    )\n",
    "                \n",
    "                # Retry with backoff\n",
    "                delay = Config.RETRY_INITIAL_DELAY * (2 ** attempt)\n",
    "                delay = min(delay, Config.RETRY_MAX_DELAY)\n",
    "                \n",
    "                logger.warning(\n",
    "                    f\"Attempt {attempt + 1}/{max_retries} failed. \"\n",
    "                    f\"Retrying in {delay}s...\"\n",
    "                )\n",
    "                time.sleep(delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Non-retriable error\n",
    "                duration_ms = (time.time() - start_time) * 1000\n",
    "                \n",
    "                self.logger.log_llm_call(\n",
    "                    prompt=prompt,\n",
    "                    response=None,\n",
    "                    duration_ms=duration_ms,\n",
    "                    success=False,\n",
    "                    error=str(e)\n",
    "                )\n",
    "                \n",
    "                return ChatResult(\n",
    "                    success=False,\n",
    "                    error=f\"Non-retriable error: {str(e)}\",\n",
    "                    error_type=\"unknown\"\n",
    "                )\n",
    "        \n",
    "        return ChatResult(\n",
    "            success=False,\n",
    "            error=\"Unexpected: exhausted retries without exception\",\n",
    "            error_type=\"unknown\"\n",
    "        )\n",
    "\n",
    "# Create production client\n",
    "prod_client = ProductionLLMClient()\n",
    "\n",
    "print(\"‚úÖ Production client initialized with:\")\n",
    "print(f\"   - Automatic retries ({Config.MAX_RETRIES})\")\n",
    "print(f\"   - Timeout management ({Config.LLM_TIMEOUT}s)\")\n",
    "print(f\"   - Structured logging\")\n",
    "print(f\"   - Error handling\")\n",
    "print(f\"   - Environment config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Production Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test successful call\n",
    "result = prod_client.chat(\"What is the capital of France?\")\n",
    "\n",
    "print(\"\\nüß™ Test Results:\\n\")\n",
    "print(f\"Success: {result.success}\")\n",
    "\n",
    "if result.success:\n",
    "    print(f\"Response: {result.response}\")\n",
    "    print(f\"Duration: {result.metadata.get('duration_ms', 0):.2f}ms\")\n",
    "    print(f\"Attempts: {result.metadata.get('attempts', 0)}\")\n",
    "else:\n",
    "    print(f\"Error Type: {result.error_type}\")\n",
    "    print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise: Build a Robust Agent Wrapper\n",
    "\n",
    "**Challenge:** Create a production-ready agent wrapper with:\n",
    "\n",
    "1. Error handling for agent failures\n",
    "2. Retry logic for transient errors\n",
    "3. Progress monitoring\n",
    "4. Timeout management\n",
    "5. Detailed logging\n",
    "\n",
    "**Requirements:**\n",
    "- Wrap the `client.react()` method\n",
    "- Handle agent failures gracefully\n",
    "- Log each step of agent execution\n",
    "- Return structured result with success/failure status\n",
    "- Support custom stop conditions\n",
    "\n",
    "Try it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to see solution</summary>\n\n```python\n# Solution: Production Agent Wrapper\n\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport time\n\n@dataclass\nclass AgentJobResult:\n    \"\"\"Result of an agent job execution.\"\"\"\n    success: bool\n    task: str\n    result: Optional[str] = None\n    error: Optional[str] = None\n    steps_taken: int = 0\n    duration_ms: float = 0\n    attempts: int = 1\n\nclass ProductionAgentRunner:\n    \"\"\"\n    Production-ready agent execution wrapper.\n    \"\"\"\n    \n    def __init__(self, client: LocalLLMClient):\n        self.client = client\n        self.logger = logging.getLogger(\"agent_runner\")\n    \n    def run_agent(\n        self,\n        task: str,\n        max_steps: int = 10,\n        max_retries: int = 2,\n        timeout: int = 300\n    ) -> AgentJobResult:\n        \"\"\"\n        Run agent task with production features.\n        \"\"\"\n        self.logger.info(f\"Starting agent task: {task[:100]}...\")\n        start_time = time.time()\n        \n        for attempt in range(max_retries):\n            try:\n                self.logger.info(f\"Attempt {attempt + 1}/{max_retries}\")\n                \n                # Run agent\n                result = self.client.react(\n                    task,\n                    max_iterations=max_steps\n                )\n                \n                duration_ms = (time.time() - start_time) * 1000\n                \n                if result.status == \"success\":\n                    self.logger.info(\n                        f\"Agent succeeded in {result.iterations} steps, \"\n                        f\"{duration_ms:.2f}ms\"\n                    )\n                    \n                    return AgentJobResult(\n                        success=True,\n                        task=task,\n                        result=result.final_response,\n                        steps_taken=result.iterations,\n                        duration_ms=duration_ms,\n                        attempts=attempt + 1\n                    )\n                else:\n                    self.logger.warning(\n                        f\"Agent did not succeed: {result.stop_reason}\"\n                    )\n                    \n                    if attempt < max_retries - 1:\n                        # Retry\n                        delay = 2 ** attempt\n                        self.logger.info(f\"Retrying in {delay}s...\")\n                        time.sleep(delay)\n                    else:\n                        # Last attempt, return failure\n                        return AgentJobResult(\n                            success=False,\n                            task=task,\n                            error=f\"Agent stopped: {result.stop_reason}\",\n                            steps_taken=result.iterations,\n                            duration_ms=duration_ms,\n                            attempts=attempt + 1\n                        )\n                        \n            except Exception as e:\n                duration_ms = (time.time() - start_time) * 1000\n                self.logger.error(f\"Agent error: {e}\")\n                \n                if attempt < max_retries - 1:\n                    delay = 2 ** attempt\n                    self.logger.info(f\"Retrying in {delay}s...\")\n                    time.sleep(delay)\n                else:\n                    return AgentJobResult(\n                        success=False,\n                        task=task,\n                        error=str(e),\n                        duration_ms=duration_ms,\n                        attempts=attempt + 1\n                    )\n        \n        return AgentJobResult(\n            success=False,\n            task=task,\n            error=\"Unexpected: exhausted retries\"\n        )\n\n# Test it\ntest_client = LocalLLMClient(\n    base_url=\"http://169.254.83.107:1234/v1\",\n    model=\"mistralai/magistral-small-2509\"\n)\n# Register built-in tools\ntest_client.register_tools_from(None)\n\nrunner = ProductionAgentRunner(test_client)\n\nresult = runner.run_agent(\n    \"Calculate 15 factorial, then count how many digits it has\",\n    max_iterations=8,\n    max_retries=2\n)\n\nprint(\"\\nü§ñ Agent Job Results:\\n\")\nprint(f\"Success: {result.success}\")\nprint(f\"Task: {result.task}\")\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Duration: {result.duration_ms:.2f}ms\")\nprint(f\"Attempts: {result.attempts}\")\n\nif result.success:\n    print(f\"\\nResult: {result.result}\")\nelse:\n    print(f\"\\nError: {result.error}\")\n```\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution cell (run to see the answer)\n",
    "# [Solution code from above would go here]\n",
    "print(\"See the solution in the dropdown above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### 1. Not Handling Specific Exceptions\n",
    "```python\n",
    "# ‚ùå Bad: Catch-all exception handling\n",
    "try:\n",
    "    response = client.chat(prompt)\n",
    "except Exception:\n",
    "    pass  # Silently fails, no insight\n",
    "\n",
    "# ‚úÖ Good: Handle specific exceptions\n",
    "try:\n",
    "    response = client.chat(prompt)\n",
    "except TimeoutError:\n",
    "    # Handle timeout specifically\n",
    "except APIError:\n",
    "    # Handle API errors specifically\n",
    "```\n",
    "\n",
    "### 2. Infinite Retries\n",
    "```python\n",
    "# ‚ùå Bad: Infinite retry loop\n",
    "while True:\n",
    "    try:\n",
    "        return client.chat(prompt)\n",
    "    except:\n",
    "        continue  # Could run forever!\n",
    "\n",
    "# ‚úÖ Good: Limited retries\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        return client.chat(prompt)\n",
    "    except:\n",
    "        if attempt == max_retries - 1:\n",
    "            raise\n",
    "```\n",
    "\n",
    "### 3. No Timeout\n",
    "```python\n",
    "# ‚ö†Ô∏è Warning: Could hang forever\n",
    "client = LocalLLMClient(base_url=\"...\", model=\"...\")\n",
    "# No timeout set!\n",
    "\n",
    "# ‚úÖ Good: Always set timeout\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"...\",\n",
    "    model=\"...\",\n",
    "    timeout=300\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Hardcoded Configuration\n",
    "```python\n",
    "# ‚ùå Bad: Hardcoded values\n",
    "client = LocalLLMClient(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"my-model\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Good: Environment-based config\n",
    "client = LocalLLMClient(\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\"),\n",
    "    model=os.getenv(\"LLM_MODEL\")\n",
    ")\n",
    "```\n",
    "\n",
    "### 5. Poor Logging\n",
    "```python\n",
    "# ‚ùå Bad: Print statements\n",
    "print(\"Calling LLM...\")\n",
    "response = client.chat(prompt)\n",
    "print(\"Done\")\n",
    "\n",
    "# ‚úÖ Good: Structured logging\n",
    "logger.info(\"Calling LLM\", extra={\"prompt_length\": len(prompt)})\n",
    "response = client.chat(prompt)\n",
    "logger.info(\"LLM call completed\", extra={\"response_length\": len(response)})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You Learned\n",
    "\n",
    "‚úÖ **Error Handling**: Comprehensive try/except patterns for different failure modes\n",
    "\n",
    "‚úÖ **Timeouts**: Preventing hanging operations with configurable timeouts\n",
    "\n",
    "‚úÖ **Retry Logic**: Exponential backoff for transient failures\n",
    "\n",
    "‚úÖ **Environment Config**: Using environment variables and .env files\n",
    "\n",
    "‚úÖ **Structured Logging**: JSON logging with metadata for production\n",
    "\n",
    "‚úÖ **Production Wrapper**: Complete production-ready client implementation\n",
    "\n",
    "‚úÖ **Best Practices**: When and how to use each pattern effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "You've mastered production patterns! Now let's apply everything you've learned in hands-on projects.\n",
    "\n",
    "‚û°Ô∏è Continue to [10-mini-project-code-helper.ipynb](./10-mini-project-code-helper.ipynb) to build:\n",
    "- A complete code review assistant agent\n",
    "- Filesystem operations for loading code\n",
    "- Code analysis with execute_python\n",
    "- Automated testing and bug detection\n",
    "- Fix suggestions and application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}